{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "##BONUS: PYTORCH LIGHTNING\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n",
    "\n",
    "seed=42 \n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y, y_pred):\n",
    "    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n",
    "    'Precision':np.round(precision_score(y, y_pred),2),\n",
    "    'Recall':np.round(recall_score(y, y_pred),2),\n",
    "    'F1':np.round(f1_score(y, y_pred),2),\n",
    "    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n",
    "    scores_df = pd.Series(data).to_frame('scores')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_icd.csv')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "train_data = df[df['split_train'] == 1]\n",
    "val_data = df[df['split_val'] == 1]\n",
    "test_data = df[df['split_test']== 1]\n",
    "\n",
    "X = df.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_val = val_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_test = test_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_val = val_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 258)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)          \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "comp= [n for n in range(len(cum_sum))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(comp, cum_sum, marker='.')\n",
    "plt.xlabel('PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, X_data, y_data, device=DEVICE):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE)\n",
    "valid_loader = DataLoader(dataset=val_data, batch_size=2)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 258\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_{}\".format(i), 8, 25)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features=out_features\n",
    "    layers.append(nn.Linear(out_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # call the define_model method\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Optimizer and loss definition\n",
    "    lr = trial.suggest_float(\"lr\", 5e-4, 1e-2, log=True)\n",
    "    optimizer =  getattr(optim, 'Adam')(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    # Using the logit binary CE, include the sigmoid function in the prediction output during the loss calculation\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        #TRAINING\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #sum all batch losses\n",
    "            #ACCURACY\n",
    "            correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(100 * correct / total) \n",
    "        train_loss.append(running_loss/total_step) #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "                #PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                #LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss+=loss_v.item()\n",
    "                #ACCURACY\n",
    "                correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss/total_step_val)\n",
    "\n",
    "        trial.report(np.mean(valid_loss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return np.mean(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-16 16:07:14,636]\u001b[0m A new study created in memory with name: no-name-33bb5b53-c453-4e6c-b61b-ea1dcc4b65c5\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:07:38,355]\u001b[0m Trial 0 finished with value: 80.53108808290156 and parameters: {'n_layers': 1, 'n_units_0': 15, 'dropout_0': 0.4998779514964801, 'lr': 0.0014030003906542051}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:08,207]\u001b[0m Trial 1 finished with value: 79.32642487046633 and parameters: {'n_layers': 2, 'n_units_0': 12, 'dropout_0': 0.39229111919427573, 'n_units_1': 8, 'dropout_1': 0.24956570215365764, 'lr': 0.0008975795901603903}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:39,496]\u001b[0m Trial 2 finished with value: 80.0 and parameters: {'n_layers': 2, 'n_units_0': 19, 'dropout_0': 0.2001710791178501, 'n_units_1': 13, 'dropout_1': 0.3056536493974983, 'lr': 0.003214341497821319}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:11,248]\u001b[0m Trial 3 finished with value: 80.01295336787564 and parameters: {'n_layers': 2, 'n_units_0': 13, 'dropout_0': 0.41694050405808236, 'n_units_1': 20, 'dropout_1': 0.46150407713224034, 'lr': 0.0008787075670310699}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:42,679]\u001b[0m Trial 4 finished with value: 79.77979274611398 and parameters: {'n_layers': 2, 'n_units_0': 15, 'dropout_0': 0.2928630597267775, 'n_units_1': 16, 'dropout_1': 0.2814550486880727, 'lr': 0.0007173910365179443}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:07,042]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:07,835]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:08,608]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:09,263]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:34,270]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:34,987]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:35,709]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:57,488]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:58,208]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:58,925]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:59,643]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:00,354]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:31,295]\u001b[0m Trial 17 finished with value: 78.83419689119171 and parameters: {'n_layers': 2, 'n_units_0': 14, 'dropout_0': 0.27437340647986463, 'n_units_1': 25, 'dropout_1': 0.4779109117693431, 'lr': 0.000988914848547741}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:31,992]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:32,784]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:33,476]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:34,263]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:35,053]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:35,829]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:36,624]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:37,412]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:38,192]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:38,969]\u001b[0m Trial 27 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:39,672]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:03,964]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:04,723]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:27,806]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:59,123]\u001b[0m Trial 32 finished with value: 79.94818652849742 and parameters: {'n_layers': 2, 'n_units_0': 13, 'dropout_0': 0.2041914340081098, 'n_units_1': 15, 'dropout_1': 0.33177274589971906, 'lr': 0.0009096506770532026}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:24,909]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:25,709]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:26,519]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:27,325]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:28,128]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:28,931]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:29,738]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:30,536]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:31,340]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:32,096]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:53,266]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:53,973]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:54,761]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:55,474]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:56,268]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:56,977]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:57,777]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:58,484]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:31,507]\u001b[0m Trial 51 finished with value: 79.11917098445596 and parameters: {'n_layers': 2, 'n_units_0': 12, 'dropout_0': 0.3905629085157136, 'n_units_1': 9, 'dropout_1': 0.25534079356351336, 'lr': 0.0008483358199006643}. Best is trial 0 with value: 80.53108808290156.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:32,372]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:33,234]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:34,103]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:34,963]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:35,817]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:36,643]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:37,362]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:59,941]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:23,349]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:24,153]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:24,953]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:25,752]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:26,553]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:27,352]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:28,152]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:28,950]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:29,747]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:30,546]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:31,251]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:32,047]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:32,849]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:33,644]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:34,436]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:35,235]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:55,971]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:56,747]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:57,548]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:20,041]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:20,846]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:21,651]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:22,459]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:23,263]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:24,071]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:24,786]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:25,588]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:26,398]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:27,209]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:28,019]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:28,832]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:29,650]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:30,458]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:31,266]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:32,074]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:32,889]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:33,603]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:34,401]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:35,212]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:36,017]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  92\n",
      "  Number of complete trials:  8\n",
      "Best trial:\n",
      "  Value:  80.53108808290156\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_0: 15\n",
      "    dropout_0: 0.4998779514964801\n",
      "    lr: 0.0014030003906542051\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "\n",
    "params = []\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    params.append(value)\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 15, 0.4998779514964801, 0.0014030003906542051]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = params[0]\n",
    "\n",
    "units_1 = params[1]\n",
    "dropout_1 = np.round(params[2],5)\n",
    "\n",
    "lr = np.round(params[3],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.Linear(X_train.shape[1], units_1)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_1)\n",
    "        self.layer_out = nn.Linear(units_1, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.layer_1(inputs))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer_1): Linear(in_features=258, out_features=15, bias=True)\n",
      "  (dropout1): Dropout(p=0.49988, inplace=False)\n",
      "  (layer_out): Linear(in_features=15, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation loss decreased (inf --> 0.577636).  Saving model ...\n",
      "\t Train_Loss: 0.5845 Train_Acc: 68.913 Val_Loss: 0.5776  BEST VAL Loss: 0.5776  Val_Acc: 67.876\n",
      "\n",
      "Epoch 1: Validation loss decreased (0.577636 --> 0.518622).  Saving model ...\n",
      "\t Train_Loss: 0.5006 Train_Acc: 83.356 Val_Loss: 0.5186  BEST VAL Loss: 0.5186  Val_Acc: 81.347\n",
      "\n",
      "Epoch 2: Validation loss decreased (0.518622 --> 0.495488).  Saving model ...\n",
      "\t Train_Loss: 0.4488 Train_Acc: 86.451 Val_Loss: 0.4955  BEST VAL Loss: 0.4955  Val_Acc: 82.383\n",
      "\n",
      "Epoch 3: Validation loss decreased (0.495488 --> 0.485232).  Saving model ...\n",
      "\t Train_Loss: 0.4160 Train_Acc: 88.514 Val_Loss: 0.4852  BEST VAL Loss: 0.4852  Val_Acc: 83.420\n",
      "\n",
      "Epoch 4: Validation loss decreased (0.485232 --> 0.479636).  Saving model ...\n",
      "\t Train_Loss: 0.3945 Train_Acc: 88.446 Val_Loss: 0.4796  BEST VAL Loss: 0.4796  Val_Acc: 82.383\n",
      "\n",
      "Epoch 5: Validation loss decreased (0.479636 --> 0.474717).  Saving model ...\n",
      "\t Train_Loss: 0.3777 Train_Acc: 89.477 Val_Loss: 0.4747  BEST VAL Loss: 0.4747  Val_Acc: 82.902\n",
      "\n",
      "Epoch 6: Validation loss decreased (0.474717 --> 0.472167).  Saving model ...\n",
      "\t Train_Loss: 0.3639 Train_Acc: 89.271 Val_Loss: 0.4722  BEST VAL Loss: 0.4722  Val_Acc: 83.420\n",
      "\n",
      "Epoch 7: Validation loss decreased (0.472167 --> 0.472076).  Saving model ...\n",
      "\t Train_Loss: 0.3540 Train_Acc: 88.996 Val_Loss: 0.4721  BEST VAL Loss: 0.4721  Val_Acc: 82.383\n",
      "\n",
      "Epoch 8: Validation loss did not decrease\n",
      "\t Train_Loss: 0.3446 Train_Acc: 90.234 Val_Loss: 0.4727  BEST VAL Loss: 0.4721  Val_Acc: 82.902\n",
      "\n",
      "Epoch 9: Validation loss did not decrease\n",
      "\t Train_Loss: 0.3352 Train_Acc: 90.371 Val_Loss: 0.4733  BEST VAL Loss: 0.4721  Val_Acc: 82.902\n",
      "\n",
      "Epoch 10: Validation loss did not decrease\n",
      "\t Train_Loss: 0.3274 Train_Acc: 90.371 Val_Loss: 0.4754  BEST VAL Loss: 0.4721  Val_Acc: 81.865\n",
      "\n",
      "Epoch 11: Validation loss did not decrease\n",
      "\t Train_Loss: 0.3211 Train_Acc: 90.234 Val_Loss: 0.4785  BEST VAL Loss: 0.4721  Val_Acc: 81.347\n",
      "\n",
      "Epoch 12: Validation loss did not decrease\n",
      "\t Train_Loss: 0.3149 Train_Acc: 90.509 Val_Loss: 0.4826  BEST VAL Loss: 0.4721  Val_Acc: 81.347\n",
      "\n",
      "Epoch 13: Validation loss did not decrease\n",
      "\t Train_Loss: 0.3088 Train_Acc: 90.853 Val_Loss: 0.4861  BEST VAL Loss: 0.4721  Val_Acc: 82.383\n",
      "\n",
      "Epoch 14: Validation loss did not decrease\n",
      "\t Train_Loss: 0.3025 Train_Acc: 91.334 Val_Loss: 0.4902  BEST VAL Loss: 0.4721  Val_Acc: 81.347\n",
      "\n",
      "Epoch 15: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2978 Train_Acc: 90.715 Val_Loss: 0.4956  BEST VAL Loss: 0.4721  Val_Acc: 80.829\n",
      "\n",
      "Epoch 16: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2947 Train_Acc: 90.578 Val_Loss: 0.5000  BEST VAL Loss: 0.4721  Val_Acc: 81.865\n",
      "\n",
      "Epoch 17: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2909 Train_Acc: 91.334 Val_Loss: 0.5049  BEST VAL Loss: 0.4721  Val_Acc: 81.865\n",
      "\n",
      "Epoch 18: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2869 Train_Acc: 91.403 Val_Loss: 0.5085  BEST VAL Loss: 0.4721  Val_Acc: 82.902\n",
      "\n",
      "Epoch 19: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2836 Train_Acc: 91.059 Val_Loss: 0.5126  BEST VAL Loss: 0.4721  Val_Acc: 82.383\n",
      "\n",
      "Epoch 20: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2806 Train_Acc: 91.953 Val_Loss: 0.5164  BEST VAL Loss: 0.4721  Val_Acc: 81.865\n",
      "\n",
      "Epoch 21: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2768 Train_Acc: 92.366 Val_Loss: 0.5207  BEST VAL Loss: 0.4721  Val_Acc: 81.347\n",
      "\n",
      "Epoch 22: Validation loss did not decrease\n",
      "\t Train_Loss: 0.2734 Train_Acc: 92.160 Val_Loss: 0.5257  BEST VAL Loss: 0.4721  Val_Acc: 80.311\n",
      "\n",
      "Epoch 23: Validation loss did not decrease\n",
      "Early stopped at epoch : 23\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "early_stopping_patience = 15\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "valid_loss_min=np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    #TRAINING\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_batch)\n",
    "        y_pred = torch.round(torch.sigmoid(output))\n",
    "        #LOSS\n",
    "        loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() #sum loss for every batch\n",
    "        #ACCURACY\n",
    "        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "        total += y_train_batch.size(0)\n",
    "    train_acc.append(100 * correct / total) #calculate accuracy among all entries in the batches\n",
    "    train_loss.append(running_loss/total_step)  #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "    # VALIDATION\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_valid_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "            batch_loss+=loss_v.item()\n",
    "            #ACCURACY\n",
    "            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "            total_v += y_valid_batch.size(0)\n",
    "        valid_acc.append(100 * correct_v / total_v) \n",
    "        valid_loss.append(batch_loss/total_step_val)\n",
    "    \n",
    "    \n",
    "    if np.mean(valid_loss) <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '/home/bmlserver/jk/iPynb/mmF_Final/savedModel/icd_state_dict.pt')\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n",
    "        valid_loss_min = np.mean(valid_loss)\n",
    "        early_stopping_counter=0 #reset counter if validation loss decreases\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
    "        early_stopping_counter+=1\n",
    "\n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', epoch)\n",
    "        break\n",
    "\n",
    "    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('/home/bmlserver/jk/iPynb/mmF_Final/savedModel/icd_state_dict.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "            X_test_batch = X_test_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_test_batch)\n",
    "            y_pred_prob = torch.sigmoid(output)\n",
    "            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            y_pred_list.append(y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_pred = [a.tolist() for a in y_pred_prob_list]\n",
    "\n",
    "icd_pred_np = np.array(icd_pred)\n",
    "\n",
    "icd_predTT = icd_pred_np.reshape((190, 1))\n",
    "\n",
    "np.savetxt('icd_pred.csv',icd_predTT,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'icd_predTT' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "icd_predTT\n",
    "\n",
    "%store icd_predTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "# y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c8a573545fa7324502f2b11294db4f50d401dc4d1e743003ac21faacdb8f11f"
  },
  "kernelspec": {
   "display_name": "jk",
   "language": "python",
   "name": "jk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
