{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "##BONUS: PYTORCH LIGHTNING\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n",
    "\n",
    "seed=42 \n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y, y_pred):\n",
    "    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n",
    "    'Precision':np.round(precision_score(y, y_pred),2),\n",
    "    'Recall':np.round(recall_score(y, y_pred),2),\n",
    "    'F1':np.round(f1_score(y, y_pred),2),\n",
    "    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n",
    "    scores_df = pd.Series(data).to_frame('scores')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_vital.csv')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "train_data = df[df['split_train'] == 1]\n",
    "val_data = df[df['split_val'] == 1]\n",
    "test_data = df[df['split_test']== 1]\n",
    "\n",
    "X = df.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_val = val_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_test = test_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_val = val_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)          \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "comp= [n for n in range(len(cum_sum))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(comp, cum_sum, marker='.')\n",
    "plt.xlabel('PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, X_data, y_data, device=DEVICE):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE)\n",
    "valid_loader = DataLoader(dataset=val_data, batch_size=2)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 9\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_{}\".format(i), 8, 25)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features=out_features\n",
    "    layers.append(nn.Linear(out_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # call the define_model method\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Optimizer and loss definition\n",
    "    lr = trial.suggest_float(\"lr\", 5e-4, 1e-2, log=True)\n",
    "    optimizer =  getattr(optim, 'Adam')(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    # Using the logit binary CE, we include the sigmoid function in the prediction output during the loss calculation\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        #TRAINING\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #sum all batch losses\n",
    "            #ACCURACY\n",
    "            correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(100 * correct / total) \n",
    "        train_loss.append(running_loss/total_step) #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "                #PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                #LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss+=loss_v.item()\n",
    "                #ACCURACY\n",
    "                correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss/total_step_val)\n",
    "\n",
    "        trial.report(np.mean(valid_loss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return np.mean(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-16 16:07:31,055]\u001b[0m A new study created in memory with name: no-name-c106886a-1934-46a1-83e6-e352dc3e068a\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:16,934]\u001b[0m Trial 0 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 20, 'dropout_0': 0.495355586309162, 'n_units_1': 21, 'dropout_1': 0.3611357656787737, 'lr': 0.0030156750166228864}. Best is trial 0 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:03,182]\u001b[0m Trial 1 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 13, 'dropout_0': 0.20566962743543235, 'n_units_1': 8, 'dropout_1': 0.48978973665535275, 'lr': 0.008855872241731715}. Best is trial 0 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:34,003]\u001b[0m Trial 2 finished with value: 43.67875647668393 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.46849730814452045, 'lr': 0.001323303324147222}. Best is trial 0 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:16,474]\u001b[0m Trial 3 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 11, 'dropout_0': 0.47774598368086096, 'n_units_1': 10, 'dropout_1': 0.2999982082470646, 'lr': 0.0053796846671346765}. Best is trial 0 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:44,963]\u001b[0m Trial 4 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 13, 'dropout_0': 0.4893362471585968, 'lr': 0.0009804660773241206}. Best is trial 0 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:12,541]\u001b[0m Trial 5 finished with value: 44.05440414507772 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.4968023654977252, 'lr': 0.009256283294335239}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:14,590]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:16,952]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:19,292]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:49,812]\u001b[0m Trial 9 finished with value: 43.93782383419689 and parameters: {'n_layers': 2, 'n_units_0': 13, 'dropout_0': 0.22187219335058153, 'n_units_1': 20, 'dropout_1': 0.24278303130354137, 'lr': 0.0027252988501780227}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:00,328]\u001b[0m Trial 10 finished with value: 43.743523316062166 and parameters: {'n_layers': 1, 'n_units_0': 17, 'dropout_0': 0.3115933352421277, 'lr': 0.009369782821290393}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:02,309]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:05,647]\u001b[0m Trial 12 finished with value: 43.95077720207254 and parameters: {'n_layers': 1, 'n_units_0': 18, 'dropout_0': 0.41224000090668766, 'lr': 0.003015885892201431}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:09,383]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:10,958]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:22:26,562]\u001b[0m Trial 15 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 8, 'dropout_0': 0.3553371629116217, 'n_units_1': 17, 'dropout_1': 0.3890024308150556, 'lr': 0.007034558810837903}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:22:27,986]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:22:29,861]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:19,910]\u001b[0m Trial 18 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 16, 'dropout_0': 0.3776603578464899, 'lr': 0.006482280268095553}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:21,591]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:22,909]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:24:30,044]\u001b[0m Trial 21 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 8, 'dropout_0': 0.20604096415679812, 'n_units_1': 8, 'dropout_1': 0.49708542106484277, 'lr': 0.009506130242972859}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:25:18,421]\u001b[0m Trial 22 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 16, 'dropout_0': 0.37211385788593515, 'lr': 0.006448195634949074}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:25:19,441]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:25:21,241]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:25:23,356]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:25:24,989]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:26:37,232]\u001b[0m Trial 27 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 18, 'dropout_0': 0.32425662575350844, 'n_units_1': 21, 'dropout_1': 0.3424481555492989, 'lr': 0.005489645707569914}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:26:38,571]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:26:40,505]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:26:41,528]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:26:42,553]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:26:44,464]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:27:48,247]\u001b[0m Trial 33 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 8, 'dropout_0': 0.22117643051784644, 'n_units_1': 17, 'dropout_1': 0.38708475156584893, 'lr': 0.007252709618218059}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:27:49,343]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:27:50,743]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:27:51,844]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:28:56,782]\u001b[0m Trial 37 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 22, 'dropout_0': 0.27455830144388055, 'n_units_1': 22, 'dropout_1': 0.3563344953307418, 'lr': 0.004502477844343681}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:29:59,162]\u001b[0m Trial 38 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 10, 'dropout_0': 0.22504660975952542, 'n_units_1': 19, 'dropout_1': 0.3839712572864168, 'lr': 0.007272671374578035}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:30:01,168]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:30:03,150]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:30:05,115]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:31:17,368]\u001b[0m Trial 42 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 21, 'dropout_0': 0.3954877754590736, 'n_units_1': 21, 'dropout_1': 0.32501338215608355, 'lr': 0.005859825734981322}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:31:19,224]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:31:21,152]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:31:23,084]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:31:25,010]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:31:26,936]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:32:05,027]\u001b[0m Trial 48 finished with value: 43.989637305699475 and parameters: {'n_layers': 1, 'n_units_0': 12, 'dropout_0': 0.3406496215870808, 'lr': 0.008484938748682029}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:32:06,955]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:32:07,951]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:33:23,137]\u001b[0m Trial 51 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 21, 'dropout_0': 0.21492321909393014, 'n_units_1': 14, 'dropout_1': 0.4935356295353732, 'lr': 0.0062392070231497435}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:33:25,111]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:33:26,061]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:33:27,932]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:33:29,773]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:33:31,671]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:33:32,633]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:19,629]\u001b[0m Trial 58 finished with value: 43.91191709844559 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.260861899954091, 'lr': 0.0087955993500908}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:21,003]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:23,060]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:24,697]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:25,888]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:27,757]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:29,567]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:31,360]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:33,146]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:34,744]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:36,499]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:34:38,260]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:35:51,256]\u001b[0m Trial 70 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 10, 'dropout_0': 0.20056644206468235, 'n_units_1': 24, 'dropout_1': 0.4319131285656683, 'lr': 0.008793223527105163}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:36:59,581]\u001b[0m Trial 71 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 21, 'dropout_0': 0.21345328558971316, 'n_units_1': 16, 'dropout_1': 0.31007790263926605, 'lr': 0.006904704722240763}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:37:01,539]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:38:21,512]\u001b[0m Trial 73 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 19, 'dropout_0': 0.3283596456396652, 'n_units_1': 20, 'dropout_1': 0.33191778958742674, 'lr': 0.005197175370374698}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:39:26,540]\u001b[0m Trial 74 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 22, 'dropout_0': 0.2157886389901032, 'n_units_1': 24, 'dropout_1': 0.48113311411417786, 'lr': 0.006817172804909724}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:40:45,035]\u001b[0m Trial 75 finished with value: 44.01554404145078 and parameters: {'n_layers': 2, 'n_units_0': 19, 'dropout_0': 0.20172692162713865, 'n_units_1': 16, 'dropout_1': 0.3165254065754697, 'lr': 0.006160411964593238}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:40:46,408]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:40:47,777]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:40:49,149]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:40:50,175]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:40:51,784]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:42:06,292]\u001b[0m Trial 81 finished with value: 44.00259067357513 and parameters: {'n_layers': 2, 'n_units_0': 8, 'dropout_0': 0.3111668468850319, 'n_units_1': 10, 'dropout_1': 0.4397542309146984, 'lr': 0.00852247657248481}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:42:08,208]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:42:09,199]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:42:10,189]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:42:12,153]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:43:25,636]\u001b[0m Trial 86 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 20, 'dropout_0': 0.20922300568911256, 'n_units_1': 23, 'dropout_1': 0.3074755922278335, 'lr': 0.007572077977036167}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:43:27,414]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:43:29,089]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:43:30,455]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:43:32,060]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:43:34,164]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:43:36,606]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:44:43,775]\u001b[0m Trial 93 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 12, 'dropout_0': 0.2458816619006861, 'n_units_1': 22, 'dropout_1': 0.3595941107871352, 'lr': 0.005799256836367107}. Best is trial 5 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:44:45,289]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:44:46,709]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:44:48,572]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:44:49,595]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:44:51,529]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:44:53,434]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  71\n",
      "  Number of complete trials:  29\n",
      "Best trial:\n",
      "  Value:  44.05440414507772\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_0: 22\n",
      "    dropout_0: 0.4968023654977252\n",
      "    lr: 0.009256283294335239\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "\n",
    "params = []\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    params.append(value)\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 22, 0.4968023654977252, 0.009256283294335239]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = params[0]\n",
    "\n",
    "units_1 = params[1]\n",
    "dropout_1 = np.round(params[2],5)\n",
    "\n",
    "lr = np.round(params[3],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.Linear(X_train.shape[1], units_1)\n",
    "        self.layer_out = nn.Linear(units_1, 1) \n",
    "        self.dropout1 = nn.Dropout(p=dropout_1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.layer_1(inputs))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer_1): Linear(in_features=9, out_features=22, bias=True)\n",
      "  (layer_out): Linear(in_features=22, out_features=1, bias=True)\n",
      "  (dropout1): Dropout(p=0.4968, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation loss decreased (inf --> 0.799633).  Saving model ...\n",
      "\t Train_Loss: 0.6605 Train_Acc: 63.755 Val_Loss: 0.7996  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 1: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6548 Train_Acc: 64.443 Val_Loss: 0.8109  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 2: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6551 Train_Acc: 64.374 Val_Loss: 0.8096  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 3: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6548 Train_Acc: 65.131 Val_Loss: 0.8077  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 4: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6555 Train_Acc: 64.856 Val_Loss: 0.8050  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 5: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6544 Train_Acc: 65.062 Val_Loss: 0.8041  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 6: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6545 Train_Acc: 64.856 Val_Loss: 0.8035  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 7: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6534 Train_Acc: 64.924 Val_Loss: 0.8014  BEST VAL Loss: 0.7996  Val_Acc: 44.041\n",
      "\n",
      "Epoch 8: Validation loss decreased (0.799633 --> 0.799490).  Saving model ...\n",
      "\t Train_Loss: 0.6527 Train_Acc: 64.856 Val_Loss: 0.7995  BEST VAL Loss: 0.7995  Val_Acc: 44.041\n",
      "\n",
      "Epoch 9: Validation loss decreased (0.799490 --> 0.797700).  Saving model ...\n",
      "\t Train_Loss: 0.6525 Train_Acc: 65.062 Val_Loss: 0.7977  BEST VAL Loss: 0.7977  Val_Acc: 44.041\n",
      "\n",
      "Epoch 10: Validation loss decreased (0.797700 --> 0.795980).  Saving model ...\n",
      "\t Train_Loss: 0.6523 Train_Acc: 64.856 Val_Loss: 0.7960  BEST VAL Loss: 0.7960  Val_Acc: 44.041\n",
      "\n",
      "Epoch 11: Validation loss decreased (0.795980 --> 0.794731).  Saving model ...\n",
      "\t Train_Loss: 0.6517 Train_Acc: 64.993 Val_Loss: 0.7947  BEST VAL Loss: 0.7947  Val_Acc: 44.041\n",
      "\n",
      "Epoch 12: Validation loss decreased (0.794731 --> 0.794094).  Saving model ...\n",
      "\t Train_Loss: 0.6514 Train_Acc: 64.856 Val_Loss: 0.7941  BEST VAL Loss: 0.7941  Val_Acc: 44.041\n",
      "\n",
      "Epoch 13: Validation loss decreased (0.794094 --> 0.793677).  Saving model ...\n",
      "\t Train_Loss: 0.6511 Train_Acc: 65.062 Val_Loss: 0.7937  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 14: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6508 Train_Acc: 64.993 Val_Loss: 0.7942  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 15: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6502 Train_Acc: 65.062 Val_Loss: 0.7964  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 16: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6505 Train_Acc: 64.443 Val_Loss: 0.7962  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 17: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6499 Train_Acc: 65.199 Val_Loss: 0.7960  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 18: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6495 Train_Acc: 65.268 Val_Loss: 0.7952  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 19: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6496 Train_Acc: 64.924 Val_Loss: 0.7948  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 20: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6496 Train_Acc: 65.062 Val_Loss: 0.7942  BEST VAL Loss: 0.7937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 21: Validation loss decreased (0.793677 --> 0.793477).  Saving model ...\n",
      "\t Train_Loss: 0.6495 Train_Acc: 64.993 Val_Loss: 0.7935  BEST VAL Loss: 0.7935  Val_Acc: 44.041\n",
      "\n",
      "Epoch 22: Validation loss decreased (0.793477 --> 0.793157).  Saving model ...\n",
      "\t Train_Loss: 0.6494 Train_Acc: 64.993 Val_Loss: 0.7932  BEST VAL Loss: 0.7932  Val_Acc: 44.041\n",
      "\n",
      "Epoch 23: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6493 Train_Acc: 64.993 Val_Loss: 0.7934  BEST VAL Loss: 0.7932  Val_Acc: 44.041\n",
      "\n",
      "Epoch 24: Validation loss decreased (0.793157 --> 0.793020).  Saving model ...\n",
      "\t Train_Loss: 0.6490 Train_Acc: 64.993 Val_Loss: 0.7930  BEST VAL Loss: 0.7930  Val_Acc: 44.041\n",
      "\n",
      "Epoch 25: Validation loss decreased (0.793020 --> 0.792857).  Saving model ...\n",
      "\t Train_Loss: 0.6489 Train_Acc: 65.268 Val_Loss: 0.7929  BEST VAL Loss: 0.7929  Val_Acc: 44.041\n",
      "\n",
      "Epoch 26: Validation loss decreased (0.792857 --> 0.792559).  Saving model ...\n",
      "\t Train_Loss: 0.6487 Train_Acc: 64.787 Val_Loss: 0.7926  BEST VAL Loss: 0.7926  Val_Acc: 44.041\n",
      "\n",
      "Epoch 27: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6488 Train_Acc: 64.993 Val_Loss: 0.7927  BEST VAL Loss: 0.7926  Val_Acc: 44.041\n",
      "\n",
      "Epoch 28: Validation loss decreased (0.792559 --> 0.792228).  Saving model ...\n",
      "\t Train_Loss: 0.6486 Train_Acc: 65.062 Val_Loss: 0.7922  BEST VAL Loss: 0.7922  Val_Acc: 44.041\n",
      "\n",
      "Epoch 29: Validation loss decreased (0.792228 --> 0.792033).  Saving model ...\n",
      "\t Train_Loss: 0.6485 Train_Acc: 64.993 Val_Loss: 0.7920  BEST VAL Loss: 0.7920  Val_Acc: 44.041\n",
      "\n",
      "Epoch 30: Validation loss decreased (0.792033 --> 0.791884).  Saving model ...\n",
      "\t Train_Loss: 0.6483 Train_Acc: 65.199 Val_Loss: 0.7919  BEST VAL Loss: 0.7919  Val_Acc: 44.041\n",
      "\n",
      "Epoch 31: Validation loss decreased (0.791884 --> 0.791549).  Saving model ...\n",
      "\t Train_Loss: 0.6483 Train_Acc: 64.856 Val_Loss: 0.7915  BEST VAL Loss: 0.7915  Val_Acc: 44.041\n",
      "\n",
      "Epoch 32: Validation loss decreased (0.791549 --> 0.791317).  Saving model ...\n",
      "\t Train_Loss: 0.6481 Train_Acc: 65.062 Val_Loss: 0.7913  BEST VAL Loss: 0.7913  Val_Acc: 44.041\n",
      "\n",
      "Epoch 33: Validation loss decreased (0.791317 --> 0.790832).  Saving model ...\n",
      "\t Train_Loss: 0.6481 Train_Acc: 65.131 Val_Loss: 0.7908  BEST VAL Loss: 0.7908  Val_Acc: 44.041\n",
      "\n",
      "Epoch 34: Validation loss decreased (0.790832 --> 0.790434).  Saving model ...\n",
      "\t Train_Loss: 0.6479 Train_Acc: 65.062 Val_Loss: 0.7904  BEST VAL Loss: 0.7904  Val_Acc: 44.041\n",
      "\n",
      "Epoch 35: Validation loss decreased (0.790434 --> 0.790099).  Saving model ...\n",
      "\t Train_Loss: 0.6478 Train_Acc: 65.062 Val_Loss: 0.7901  BEST VAL Loss: 0.7901  Val_Acc: 44.041\n",
      "\n",
      "Epoch 36: Validation loss decreased (0.790099 --> 0.789721).  Saving model ...\n",
      "\t Train_Loss: 0.6477 Train_Acc: 64.993 Val_Loss: 0.7897  BEST VAL Loss: 0.7897  Val_Acc: 44.041\n",
      "\n",
      "Epoch 37: Validation loss decreased (0.789721 --> 0.789649).  Saving model ...\n",
      "\t Train_Loss: 0.6475 Train_Acc: 65.131 Val_Loss: 0.7896  BEST VAL Loss: 0.7896  Val_Acc: 44.041\n",
      "\n",
      "Epoch 38: Validation loss decreased (0.789649 --> 0.789201).  Saving model ...\n",
      "\t Train_Loss: 0.6474 Train_Acc: 65.062 Val_Loss: 0.7892  BEST VAL Loss: 0.7892  Val_Acc: 44.041\n",
      "\n",
      "Epoch 39: Validation loss decreased (0.789201 --> 0.788956).  Saving model ...\n",
      "\t Train_Loss: 0.6474 Train_Acc: 64.924 Val_Loss: 0.7890  BEST VAL Loss: 0.7890  Val_Acc: 44.041\n",
      "\n",
      "Epoch 40: Validation loss decreased (0.788956 --> 0.788760).  Saving model ...\n",
      "\t Train_Loss: 0.6473 Train_Acc: 64.993 Val_Loss: 0.7888  BEST VAL Loss: 0.7888  Val_Acc: 44.041\n",
      "\n",
      "Epoch 41: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6472 Train_Acc: 65.062 Val_Loss: 0.7889  BEST VAL Loss: 0.7888  Val_Acc: 44.041\n",
      "\n",
      "Epoch 42: Validation loss decreased (0.788760 --> 0.788525).  Saving model ...\n",
      "\t Train_Loss: 0.6471 Train_Acc: 65.062 Val_Loss: 0.7885  BEST VAL Loss: 0.7885  Val_Acc: 44.041\n",
      "\n",
      "Epoch 43: Validation loss decreased (0.788525 --> 0.788467).  Saving model ...\n",
      "\t Train_Loss: 0.6471 Train_Acc: 65.062 Val_Loss: 0.7885  BEST VAL Loss: 0.7885  Val_Acc: 44.041\n",
      "\n",
      "Epoch 44: Validation loss decreased (0.788467 --> 0.788352).  Saving model ...\n",
      "\t Train_Loss: 0.6470 Train_Acc: 64.924 Val_Loss: 0.7884  BEST VAL Loss: 0.7884  Val_Acc: 44.041\n",
      "\n",
      "Epoch 45: Validation loss decreased (0.788352 --> 0.788008).  Saving model ...\n",
      "\t Train_Loss: 0.6469 Train_Acc: 65.131 Val_Loss: 0.7880  BEST VAL Loss: 0.7880  Val_Acc: 44.041\n",
      "\n",
      "Epoch 46: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6469 Train_Acc: 64.924 Val_Loss: 0.7886  BEST VAL Loss: 0.7880  Val_Acc: 44.041\n",
      "\n",
      "Epoch 47: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6470 Train_Acc: 65.131 Val_Loss: 0.7885  BEST VAL Loss: 0.7880  Val_Acc: 44.041\n",
      "\n",
      "Epoch 48: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6468 Train_Acc: 65.131 Val_Loss: 0.7883  BEST VAL Loss: 0.7880  Val_Acc: 44.041\n",
      "\n",
      "Epoch 49: Validation loss decreased (0.788008 --> 0.787964).  Saving model ...\n",
      "\t Train_Loss: 0.6467 Train_Acc: 65.131 Val_Loss: 0.7880  BEST VAL Loss: 0.7880  Val_Acc: 44.041\n",
      "\n",
      "Epoch 50: Validation loss decreased (0.787964 --> 0.787904).  Saving model ...\n",
      "\t Train_Loss: 0.6465 Train_Acc: 65.131 Val_Loss: 0.7879  BEST VAL Loss: 0.7879  Val_Acc: 44.041\n",
      "\n",
      "Epoch 51: Validation loss decreased (0.787904 --> 0.787718).  Saving model ...\n",
      "\t Train_Loss: 0.6465 Train_Acc: 64.993 Val_Loss: 0.7877  BEST VAL Loss: 0.7877  Val_Acc: 44.041\n",
      "\n",
      "Epoch 52: Validation loss decreased (0.787718 --> 0.787461).  Saving model ...\n",
      "\t Train_Loss: 0.6465 Train_Acc: 65.131 Val_Loss: 0.7875  BEST VAL Loss: 0.7875  Val_Acc: 44.041\n",
      "\n",
      "Epoch 53: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6463 Train_Acc: 64.993 Val_Loss: 0.7881  BEST VAL Loss: 0.7875  Val_Acc: 44.041\n",
      "\n",
      "Epoch 54: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6464 Train_Acc: 64.856 Val_Loss: 0.7879  BEST VAL Loss: 0.7875  Val_Acc: 44.041\n",
      "\n",
      "Epoch 55: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6464 Train_Acc: 65.062 Val_Loss: 0.7877  BEST VAL Loss: 0.7875  Val_Acc: 44.041\n",
      "\n",
      "Epoch 56: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6462 Train_Acc: 65.131 Val_Loss: 0.7875  BEST VAL Loss: 0.7875  Val_Acc: 44.041\n",
      "\n",
      "Epoch 57: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6462 Train_Acc: 65.199 Val_Loss: 0.7876  BEST VAL Loss: 0.7875  Val_Acc: 44.041\n",
      "\n",
      "Epoch 58: Validation loss decreased (0.787461 --> 0.787404).  Saving model ...\n",
      "\t Train_Loss: 0.6461 Train_Acc: 65.062 Val_Loss: 0.7874  BEST VAL Loss: 0.7874  Val_Acc: 44.041\n",
      "\n",
      "Epoch 59: Validation loss decreased (0.787404 --> 0.787258).  Saving model ...\n",
      "\t Train_Loss: 0.6460 Train_Acc: 64.993 Val_Loss: 0.7873  BEST VAL Loss: 0.7873  Val_Acc: 44.041\n",
      "\n",
      "Epoch 60: Validation loss decreased (0.787258 --> 0.787154).  Saving model ...\n",
      "\t Train_Loss: 0.6460 Train_Acc: 65.062 Val_Loss: 0.7872  BEST VAL Loss: 0.7872  Val_Acc: 44.041\n",
      "\n",
      "Epoch 61: Validation loss decreased (0.787154 --> 0.786940).  Saving model ...\n",
      "\t Train_Loss: 0.6459 Train_Acc: 65.199 Val_Loss: 0.7869  BEST VAL Loss: 0.7869  Val_Acc: 44.041\n",
      "\n",
      "Epoch 62: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6458 Train_Acc: 65.268 Val_Loss: 0.7870  BEST VAL Loss: 0.7869  Val_Acc: 44.041\n",
      "\n",
      "Epoch 63: Validation loss decreased (0.786940 --> 0.786900).  Saving model ...\n",
      "\t Train_Loss: 0.6457 Train_Acc: 65.131 Val_Loss: 0.7869  BEST VAL Loss: 0.7869  Val_Acc: 44.041\n",
      "\n",
      "Epoch 64: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6457 Train_Acc: 65.062 Val_Loss: 0.7869  BEST VAL Loss: 0.7869  Val_Acc: 44.041\n",
      "\n",
      "Epoch 65: Validation loss decreased (0.786900 --> 0.786867).  Saving model ...\n",
      "\t Train_Loss: 0.6457 Train_Acc: 65.199 Val_Loss: 0.7869  BEST VAL Loss: 0.7869  Val_Acc: 44.041\n",
      "\n",
      "Epoch 66: Validation loss decreased (0.786867 --> 0.786767).  Saving model ...\n",
      "\t Train_Loss: 0.6456 Train_Acc: 65.199 Val_Loss: 0.7868  BEST VAL Loss: 0.7868  Val_Acc: 44.041\n",
      "\n",
      "Epoch 67: Validation loss decreased (0.786767 --> 0.786655).  Saving model ...\n",
      "\t Train_Loss: 0.6455 Train_Acc: 65.406 Val_Loss: 0.7867  BEST VAL Loss: 0.7867  Val_Acc: 44.041\n",
      "\n",
      "Epoch 68: Validation loss decreased (0.786655 --> 0.786618).  Saving model ...\n",
      "\t Train_Loss: 0.6454 Train_Acc: 65.337 Val_Loss: 0.7866  BEST VAL Loss: 0.7866  Val_Acc: 44.041\n",
      "\n",
      "Epoch 69: Validation loss decreased (0.786618 --> 0.786570).  Saving model ...\n",
      "\t Train_Loss: 0.6454 Train_Acc: 65.337 Val_Loss: 0.7866  BEST VAL Loss: 0.7866  Val_Acc: 44.041\n",
      "\n",
      "Epoch 70: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6453 Train_Acc: 65.475 Val_Loss: 0.7866  BEST VAL Loss: 0.7866  Val_Acc: 44.041\n",
      "\n",
      "Epoch 71: Validation loss decreased (0.786570 --> 0.786554).  Saving model ...\n",
      "\t Train_Loss: 0.6452 Train_Acc: 65.337 Val_Loss: 0.7866  BEST VAL Loss: 0.7866  Val_Acc: 44.041\n",
      "\n",
      "Epoch 72: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6452 Train_Acc: 64.993 Val_Loss: 0.7866  BEST VAL Loss: 0.7866  Val_Acc: 44.041\n",
      "\n",
      "Epoch 73: Validation loss decreased (0.786554 --> 0.786465).  Saving model ...\n",
      "\t Train_Loss: 0.6451 Train_Acc: 65.062 Val_Loss: 0.7865  BEST VAL Loss: 0.7865  Val_Acc: 44.041\n",
      "\n",
      "Epoch 74: Validation loss decreased (0.786465 --> 0.786414).  Saving model ...\n",
      "\t Train_Loss: 0.6451 Train_Acc: 65.131 Val_Loss: 0.7864  BEST VAL Loss: 0.7864  Val_Acc: 44.041\n",
      "\n",
      "Epoch 75: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6452 Train_Acc: 65.131 Val_Loss: 0.7864  BEST VAL Loss: 0.7864  Val_Acc: 44.041\n",
      "\n",
      "Epoch 76: Validation loss decreased (0.786414 --> 0.786336).  Saving model ...\n",
      "\t Train_Loss: 0.6452 Train_Acc: 64.993 Val_Loss: 0.7863  BEST VAL Loss: 0.7863  Val_Acc: 44.041\n",
      "\n",
      "Epoch 77: Validation loss decreased (0.786336 --> 0.786270).  Saving model ...\n",
      "\t Train_Loss: 0.6452 Train_Acc: 65.062 Val_Loss: 0.7863  BEST VAL Loss: 0.7863  Val_Acc: 44.041\n",
      "\n",
      "Epoch 78: Validation loss decreased (0.786270 --> 0.786249).  Saving model ...\n",
      "\t Train_Loss: 0.6451 Train_Acc: 65.199 Val_Loss: 0.7862  BEST VAL Loss: 0.7862  Val_Acc: 44.041\n",
      "\n",
      "Epoch 79: Validation loss decreased (0.786249 --> 0.786215).  Saving model ...\n",
      "\t Train_Loss: 0.6452 Train_Acc: 65.062 Val_Loss: 0.7862  BEST VAL Loss: 0.7862  Val_Acc: 44.041\n",
      "\n",
      "Epoch 80: Validation loss decreased (0.786215 --> 0.786178).  Saving model ...\n",
      "\t Train_Loss: 0.6451 Train_Acc: 64.993 Val_Loss: 0.7862  BEST VAL Loss: 0.7862  Val_Acc: 44.041\n",
      "\n",
      "Epoch 81: Validation loss decreased (0.786178 --> 0.786126).  Saving model ...\n",
      "\t Train_Loss: 0.6450 Train_Acc: 65.131 Val_Loss: 0.7861  BEST VAL Loss: 0.7861  Val_Acc: 44.041\n",
      "\n",
      "Epoch 82: Validation loss decreased (0.786126 --> 0.786069).  Saving model ...\n",
      "\t Train_Loss: 0.6450 Train_Acc: 65.062 Val_Loss: 0.7861  BEST VAL Loss: 0.7861  Val_Acc: 44.041\n",
      "\n",
      "Epoch 83: Validation loss decreased (0.786069 --> 0.786051).  Saving model ...\n",
      "\t Train_Loss: 0.6450 Train_Acc: 64.993 Val_Loss: 0.7861  BEST VAL Loss: 0.7861  Val_Acc: 44.041\n",
      "\n",
      "Epoch 84: Validation loss decreased (0.786051 --> 0.785922).  Saving model ...\n",
      "\t Train_Loss: 0.6450 Train_Acc: 65.199 Val_Loss: 0.7859  BEST VAL Loss: 0.7859  Val_Acc: 44.041\n",
      "\n",
      "Epoch 85: Validation loss decreased (0.785922 --> 0.785810).  Saving model ...\n",
      "\t Train_Loss: 0.6449 Train_Acc: 65.131 Val_Loss: 0.7858  BEST VAL Loss: 0.7858  Val_Acc: 44.041\n",
      "\n",
      "Epoch 86: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6449 Train_Acc: 65.199 Val_Loss: 0.7858  BEST VAL Loss: 0.7858  Val_Acc: 44.041\n",
      "\n",
      "Epoch 87: Validation loss decreased (0.785810 --> 0.785749).  Saving model ...\n",
      "\t Train_Loss: 0.6448 Train_Acc: 64.993 Val_Loss: 0.7857  BEST VAL Loss: 0.7857  Val_Acc: 44.041\n",
      "\n",
      "Epoch 88: Validation loss decreased (0.785749 --> 0.785675).  Saving model ...\n",
      "\t Train_Loss: 0.6449 Train_Acc: 64.993 Val_Loss: 0.7857  BEST VAL Loss: 0.7857  Val_Acc: 44.041\n",
      "\n",
      "Epoch 89: Validation loss decreased (0.785675 --> 0.785604).  Saving model ...\n",
      "\t Train_Loss: 0.6448 Train_Acc: 65.062 Val_Loss: 0.7856  BEST VAL Loss: 0.7856  Val_Acc: 44.041\n",
      "\n",
      "Epoch 90: Validation loss decreased (0.785604 --> 0.785563).  Saving model ...\n",
      "\t Train_Loss: 0.6448 Train_Acc: 65.131 Val_Loss: 0.7856  BEST VAL Loss: 0.7856  Val_Acc: 44.041\n",
      "\n",
      "Epoch 91: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6448 Train_Acc: 65.062 Val_Loss: 0.7856  BEST VAL Loss: 0.7856  Val_Acc: 44.041\n",
      "\n",
      "Epoch 92: Validation loss decreased (0.785563 --> 0.785547).  Saving model ...\n",
      "\t Train_Loss: 0.6447 Train_Acc: 65.131 Val_Loss: 0.7855  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n",
      "Epoch 93: Validation loss decreased (0.785547 --> 0.785512).  Saving model ...\n",
      "\t Train_Loss: 0.6447 Train_Acc: 65.131 Val_Loss: 0.7855  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n",
      "Epoch 94: Validation loss decreased (0.785512 --> 0.785472).  Saving model ...\n",
      "\t Train_Loss: 0.6447 Train_Acc: 65.131 Val_Loss: 0.7855  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n",
      "Epoch 95: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6447 Train_Acc: 65.199 Val_Loss: 0.7856  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n",
      "Epoch 96: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6447 Train_Acc: 65.131 Val_Loss: 0.7856  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n",
      "Epoch 97: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6446 Train_Acc: 65.199 Val_Loss: 0.7855  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n",
      "Epoch 98: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6446 Train_Acc: 65.131 Val_Loss: 0.7858  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n",
      "Epoch 99: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6446 Train_Acc: 65.131 Val_Loss: 0.7858  BEST VAL Loss: 0.7855  Val_Acc: 44.041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "early_stopping_patience = 15\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "valid_loss_min=np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    #TRAINING\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_batch)\n",
    "        y_pred = torch.round(torch.sigmoid(output))\n",
    "        #LOSS\n",
    "        loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() #sum loss for every batch\n",
    "        #ACCURACY\n",
    "        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "        total += y_train_batch.size(0)\n",
    "    train_acc.append(100 * correct / total) #calculate accuracy among all entries in the batches\n",
    "    train_loss.append(running_loss/total_step)  #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "    # VALIDATION\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_valid_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "            batch_loss+=loss_v.item()\n",
    "            #ACCURACY\n",
    "            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "            total_v += y_valid_batch.size(0)\n",
    "        valid_acc.append(100 * correct_v / total_v) \n",
    "        valid_loss.append(batch_loss/total_step_val)\n",
    "    \n",
    "    \n",
    "    if np.mean(valid_loss) <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '/home/bmlserver/jk/iPynb/mmF_Final/savedModel/vital_state_dict.pt')\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n",
    "        valid_loss_min = np.mean(valid_loss)\n",
    "        early_stopping_counter=0 #reset counter if validation loss decreases\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
    "        early_stopping_counter+=1\n",
    "\n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', epoch)\n",
    "        break\n",
    "\n",
    "    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('/home/bmlserver/jk/iPynb/mmF_Final/savedModel/vital_state_dict.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "            X_test_batch = X_test_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_test_batch)\n",
    "            y_pred_prob = torch.sigmoid(output)\n",
    "            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            y_pred_list.append(y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vital_pred = [a.tolist() for a in y_pred_prob_list]\n",
    "\n",
    "vital_pred_np = np.array(vital_pred)\n",
    "\n",
    "vital_predTT = vital_pred_np.reshape((190, 1))\n",
    "\n",
    "np.savetxt('vital_pred.csv',vital_predTT,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "# y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c8a573545fa7324502f2b11294db4f50d401dc4d1e743003ac21faacdb8f11f"
  },
  "kernelspec": {
   "display_name": "jk",
   "language": "python",
   "name": "jk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
