{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "##BONUS: PYTORCH LIGHTNING\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n",
    "\n",
    "seed=42 \n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y, y_pred):\n",
    "    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n",
    "    'Precision':np.round(precision_score(y, y_pred),2),\n",
    "    'Recall':np.round(recall_score(y, y_pred),2),\n",
    "    'F1':np.round(f1_score(y, y_pred),2),\n",
    "    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n",
    "    scores_df = pd.Series(data).to_frame('scores')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_demo.csv')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "train_data = df[df['split_train'] == 1]\n",
    "val_data = df[df['split_val'] == 1]\n",
    "test_data = df[df['split_test']== 1]\n",
    "\n",
    "X = df.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_val = val_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_test = test_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_val = val_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)          \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "comp= [n for n in range(len(cum_sum))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(comp, cum_sum, marker='.')\n",
    "plt.xlabel('PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, X_data, y_data, device=DEVICE):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE)\n",
    "valid_loader = DataLoader(dataset=val_data, batch_size=2)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 12\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_{}\".format(i), 8, 25)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features=out_features\n",
    "    layers.append(nn.Linear(out_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # call the define_model method\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Optimizer and loss definition\n",
    "    lr = trial.suggest_float(\"lr\", 5e-4, 1e-2, log=True)\n",
    "    optimizer =  getattr(optim, 'Adam')(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    # Using the logit binary CE, include the sigmoid function in the prediction output during the loss calculation\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        #TRAINING\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #sum all batch losses\n",
    "            #ACCURACY\n",
    "            correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(100 * correct / total) \n",
    "        train_loss.append(running_loss/total_step) #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "                #PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                #LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss+=loss_v.item()\n",
    "                #ACCURACY\n",
    "                correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss/total_step_val)\n",
    "\n",
    "        trial.report(np.mean(valid_loss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return np.mean(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-16 16:06:06,519]\u001b[0m A new study created in memory with name: no-name-52a011c9-e43f-4f77-be77-2d02982bf8ce\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:06:30,294]\u001b[0m Trial 0 finished with value: 44.02849740932643 and parameters: {'n_layers': 1, 'n_units_0': 16, 'dropout_0': 0.21121473150780767, 'lr': 0.0015559416594385538}. Best is trial 0 with value: 44.02849740932643.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:06:56,744]\u001b[0m Trial 1 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 24, 'dropout_0': 0.3199428631954207, 'n_units_1': 23, 'dropout_1': 0.46338914453626673, 'lr': 0.004980404326699433}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:07:18,142]\u001b[0m Trial 2 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 23, 'dropout_0': 0.32432657331792036, 'n_units_1': 11, 'dropout_1': 0.2958269626563271, 'lr': 0.0008993451403489508}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:07:39,612]\u001b[0m Trial 3 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 15, 'dropout_0': 0.2616399709245493, 'lr': 0.0015148114030363472}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:09,408]\u001b[0m Trial 4 finished with value: 44.02849740932643 and parameters: {'n_layers': 2, 'n_units_0': 13, 'dropout_0': 0.49130859984028236, 'n_units_1': 12, 'dropout_1': 0.3737829402179793, 'lr': 0.0007499141537608275}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:15,418]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:17,760]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:27,173]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:27,898]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:59,395]\u001b[0m Trial 9 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 11, 'dropout_0': 0.2566854262385462, 'n_units_1': 8, 'dropout_1': 0.364479394239699, 'lr': 0.006439741088149661}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:27,478]\u001b[0m Trial 10 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.46028843582453954, 'lr': 0.00967125709034148}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:58,599]\u001b[0m Trial 11 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.31109512958310004, 'n_units_1': 21, 'dropout_1': 0.49026838377097076, 'lr': 0.003989558128049123}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:59,416]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:00,214]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:30,966]\u001b[0m Trial 14 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 23, 'dropout_0': 0.3141513116858908, 'n_units_1': 21, 'dropout_1': 0.43252646249703314, 'lr': 0.005588706097539317}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:31,764]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:01,646]\u001b[0m Trial 16 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 23, 'dropout_0': 0.34420173937382625, 'n_units_1': 20, 'dropout_1': 0.42958680169799857, 'lr': 0.0023498138169760307}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:02,366]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:03,132]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:04,696]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:06,100]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:33,880]\u001b[0m Trial 21 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.4334331811979921, 'lr': 0.0097565198401073}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:35,255]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:04,865]\u001b[0m Trial 23 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.3135105663295695, 'n_units_1': 22, 'dropout_1': 0.49836666258951295, 'lr': 0.003801861143121964}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:05,571]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:33,031]\u001b[0m Trial 25 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.4055472148840334, 'lr': 0.009335319684242801}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:33,739]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:04,886]\u001b[0m Trial 27 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 24, 'dropout_0': 0.466051961269421, 'n_units_1': 13, 'dropout_1': 0.2312967833829419, 'lr': 0.0076935871470529965}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:05,704]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:06,513]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:07,336]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:08,135]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:35,985]\u001b[0m Trial 32 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.45744372969163904, 'lr': 0.009390600643552953}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:06,639]\u001b[0m Trial 33 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.4078843458476115, 'n_units_1': 18, 'dropout_1': 0.3285429585173299, 'lr': 0.007589494029905641}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:07,448]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:08,171]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:35,649]\u001b[0m Trial 36 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.4200344038036827, 'lr': 0.008913042330998952}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:36,349]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:37,051]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:37,750]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:38,447]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:07,738]\u001b[0m Trial 41 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 24, 'dropout_0': 0.4621855042770154, 'n_units_1': 20, 'dropout_1': 0.40734289110771443, 'lr': 0.007881681243139043}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:08,514]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:09,232]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:09,992]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:10,700]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:38,231]\u001b[0m Trial 46 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.44908463294296314, 'lr': 0.00981778486041652}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:08,252]\u001b[0m Trial 47 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.4138611352475612, 'n_units_1': 18, 'dropout_1': 0.3245798207127427, 'lr': 0.007064266040724804}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:38,420]\u001b[0m Trial 48 finished with value: 44.02849740932643 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.38333684249896727, 'n_units_1': 18, 'dropout_1': 0.4004138685317984, 'lr': 0.006245511447899332}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:39,173]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:07,817]\u001b[0m Trial 50 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 23, 'dropout_0': 0.3204497940093528, 'n_units_1': 21, 'dropout_1': 0.42589601405455335, 'lr': 0.0034198386551112228}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:31,588]\u001b[0m Trial 51 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.41022423633668154, 'lr': 0.00706399864415159}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:55,412]\u001b[0m Trial 52 finished with value: 44.02849740932643 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.41139008654130094, 'lr': 0.008510022350431268}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:19,752]\u001b[0m Trial 53 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 18, 'dropout_0': 0.47312829517905947, 'lr': 0.00997198125124426}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:42,050]\u001b[0m Trial 54 finished with value: 44.04145077720207 and parameters: {'n_layers': 1, 'n_units_0': 18, 'dropout_0': 0.47909939431609627, 'lr': 0.007348559184097264}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:06,212]\u001b[0m Trial 55 finished with value: 44.02849740932643 and parameters: {'n_layers': 2, 'n_units_0': 24, 'dropout_0': 0.4438640186257257, 'n_units_1': 14, 'dropout_1': 0.27837757959078085, 'lr': 0.006866668278743728}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:06,749]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:07,299]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:07,948]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:31,602]\u001b[0m Trial 59 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 23, 'dropout_0': 0.42085065771401187, 'n_units_1': 19, 'dropout_1': 0.23065189691698962, 'lr': 0.0043677022064102}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:32,130]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:32,672]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:55,819]\u001b[0m Trial 62 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.317306302285981, 'n_units_1': 22, 'dropout_1': 0.46841789525391453, 'lr': 0.0051918946188007735}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:19,098]\u001b[0m Trial 63 finished with value: 44.02849740932643 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.4344358490717765, 'n_units_1': 22, 'dropout_1': 0.4706831698305409, 'lr': 0.005031558714858755}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:19,572]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:20,048]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:38,380]\u001b[0m Trial 66 finished with value: 44.01554404145078 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.4994473797705154, 'lr': 0.009936443287015482}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:56,619]\u001b[0m Trial 67 finished with value: 44.02849740932643 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.4485159268773702, 'lr': 0.008943879311918179}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:57,090]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:17,284]\u001b[0m Trial 69 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 25, 'dropout_0': 0.336264218115036, 'n_units_1': 17, 'dropout_1': 0.32115855379473834, 'lr': 0.007339223817745066}. Best is trial 1 with value: 44.04145077720207.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:17,904]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:18,471]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:19,049]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:19,626]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:20,238]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:41,542]\u001b[0m Trial 75 finished with value: 44.05440414507772 and parameters: {'n_layers': 2, 'n_units_0': 23, 'dropout_0': 0.305699107466703, 'n_units_1': 20, 'dropout_1': 0.42153902383312125, 'lr': 0.005511574097131775}. Best is trial 75 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:42,340]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:42,827]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:43,339]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:43,815]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:44,420]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:45,010]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:45,648]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:22:10,155]\u001b[0m Trial 83 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 24, 'dropout_0': 0.4282772315132107, 'n_units_1': 24, 'dropout_1': 0.349203661396781, 'lr': 0.008641015605073354}. Best is trial 75 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:22:36,044]\u001b[0m Trial 84 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 23, 'dropout_0': 0.46341423221200173, 'n_units_1': 16, 'dropout_1': 0.37050953159379835, 'lr': 0.00801970391088983}. Best is trial 75 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:22:59,628]\u001b[0m Trial 85 finished with value: 44.02849740932643 and parameters: {'n_layers': 2, 'n_units_0': 24, 'dropout_0': 0.46356596095836833, 'n_units_1': 16, 'dropout_1': 0.34953666978757525, 'lr': 0.008538186701028024}. Best is trial 75 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:00,286]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:00,877]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:01,485]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:02,099]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:02,685]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:03,264]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:03,865]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:04,346]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:04,832]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:05,456]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:05,935]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:31,433]\u001b[0m Trial 97 finished with value: 44.04145077720207 and parameters: {'n_layers': 2, 'n_units_0': 22, 'dropout_0': 0.45257147645452, 'n_units_1': 20, 'dropout_1': 0.4287247110568553, 'lr': 0.009318189344363341}. Best is trial 75 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:57,487]\u001b[0m Trial 98 finished with value: 44.01554404145078 and parameters: {'n_layers': 2, 'n_units_0': 21, 'dropout_0': 0.4532664394776921, 'n_units_1': 12, 'dropout_1': 0.20060448625949504, 'lr': 0.009243987850581727}. Best is trial 75 with value: 44.05440414507772.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:23:58,032]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  61\n",
      "  Number of complete trials:  39\n",
      "Best trial:\n",
      "  Value:  44.05440414507772\n",
      "  Params: \n",
      "    n_layers: 2\n",
      "    n_units_0: 23\n",
      "    dropout_0: 0.305699107466703\n",
      "    n_units_1: 20\n",
      "    dropout_1: 0.42153902383312125\n",
      "    lr: 0.005511574097131775\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "\n",
    "params = []\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    params.append(value)\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 23, 0.305699107466703, 20, 0.42153902383312125, 0.005511574097131775]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = params[0]\n",
    "\n",
    "units_1 = params[1]\n",
    "dropout_1 = np.round(params[2],5)\n",
    "\n",
    "lr = np.round(params[3],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.Linear(X_train.shape[1], units_1)\n",
    "        self.layer_out = nn.Linear(units_1, 1) \n",
    "        self.dropout1 = nn.Dropout(p=dropout_1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.layer_1(inputs))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer_1): Linear(in_features=12, out_features=23, bias=True)\n",
      "  (layer_out): Linear(in_features=23, out_features=1, bias=True)\n",
      "  (dropout1): Dropout(p=0.3057, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation loss decreased (inf --> 165666.864305).  Saving model ...\n",
      "\t Train_Loss: 4379.7657 Train_Acc: 57.497 Val_Loss: 165666.8643  BEST VAL Loss: 165666.8643  Val_Acc: 44.041\n",
      "\n",
      "Epoch 1: Validation loss decreased (165666.864305 --> 114660.584817).  Saving model ...\n",
      "\t Train_Loss: 13638.4895 Train_Acc: 55.777 Val_Loss: 114660.5848  BEST VAL Loss: 114660.5848  Val_Acc: 44.041\n",
      "\n",
      "Epoch 2: Validation loss did not decrease\n",
      "\t Train_Loss: 14930.4520 Train_Acc: 55.502 Val_Loss: 166632.5699  BEST VAL Loss: 114660.5848  Val_Acc: 44.041\n",
      "\n",
      "Epoch 3: Validation loss did not decrease\n",
      "\t Train_Loss: 11231.8539 Train_Acc: 54.539 Val_Loss: 157155.3964  BEST VAL Loss: 114660.5848  Val_Acc: 44.041\n",
      "\n",
      "Epoch 4: Validation loss did not decrease\n",
      "\t Train_Loss: 8985.7497 Train_Acc: 56.327 Val_Loss: 138146.9514  BEST VAL Loss: 114660.5848  Val_Acc: 44.041\n",
      "\n",
      "Epoch 5: Validation loss did not decrease\n",
      "\t Train_Loss: 7488.3783 Train_Acc: 55.296 Val_Loss: 120117.7172  BEST VAL Loss: 114660.5848  Val_Acc: 44.041\n",
      "\n",
      "Epoch 6: Validation loss decreased (114660.584817 --> 105024.100449).  Saving model ...\n",
      "\t Train_Loss: 6418.8179 Train_Acc: 55.777 Val_Loss: 105024.1004  BEST VAL Loss: 105024.1004  Val_Acc: 55.959\n",
      "\n",
      "Epoch 7: Validation loss decreased (105024.100449 --> 92768.444691).  Saving model ...\n",
      "\t Train_Loss: 5616.6454 Train_Acc: 55.708 Val_Loss: 92768.4447  BEST VAL Loss: 92768.4447  Val_Acc: 44.041\n",
      "\n",
      "Epoch 8: Validation loss decreased (92768.444691 --> 82835.095856).  Saving model ...\n",
      "\t Train_Loss: 4992.7193 Train_Acc: 56.327 Val_Loss: 82835.0959  BEST VAL Loss: 82835.0959  Val_Acc: 44.041\n",
      "\n",
      "Epoch 9: Validation loss decreased (82835.095856 --> 74714.239060).  Saving model ...\n",
      "\t Train_Loss: 4493.5982 Train_Acc: 55.089 Val_Loss: 74714.2391  BEST VAL Loss: 74714.2391  Val_Acc: 44.041\n",
      "\n",
      "Epoch 10: Validation loss decreased (74714.239060 --> 67993.368996).  Saving model ...\n",
      "\t Train_Loss: 4085.2298 Train_Acc: 55.365 Val_Loss: 67993.3690  BEST VAL Loss: 67993.3690  Val_Acc: 44.041\n",
      "\n",
      "Epoch 11: Validation loss decreased (67993.368996 --> 62358.850670).  Saving model ...\n",
      "\t Train_Loss: 3744.9028 Train_Acc: 56.052 Val_Loss: 62358.8507  BEST VAL Loss: 62358.8507  Val_Acc: 44.041\n",
      "\n",
      "Epoch 12: Validation loss decreased (62358.850670 --> 57576.115912).  Saving model ...\n",
      "\t Train_Loss: 3456.9488 Train_Acc: 54.608 Val_Loss: 57576.1159  BEST VAL Loss: 57576.1159  Val_Acc: 44.041\n",
      "\n",
      "Epoch 13: Validation loss decreased (57576.115912 --> 53469.891856).  Saving model ...\n",
      "\t Train_Loss: 3210.1280 Train_Acc: 55.708 Val_Loss: 53469.8919  BEST VAL Loss: 53469.8919  Val_Acc: 44.041\n",
      "\n",
      "Epoch 14: Validation loss decreased (53469.891856 --> 49908.121974).  Saving model ...\n",
      "\t Train_Loss: 2996.2179 Train_Acc: 55.640 Val_Loss: 49908.1220  BEST VAL Loss: 49908.1220  Val_Acc: 44.041\n",
      "\n",
      "Epoch 15: Validation loss decreased (49908.121974 --> 46790.193858).  Saving model ...\n",
      "\t Train_Loss: 2809.0470 Train_Acc: 55.296 Val_Loss: 46790.1939  BEST VAL Loss: 46790.1939  Val_Acc: 44.041\n",
      "\n",
      "Epoch 16: Validation loss decreased (46790.193858 --> 44038.463543).  Saving model ...\n",
      "\t Train_Loss: 2643.8941 Train_Acc: 55.708 Val_Loss: 44038.4635  BEST VAL Loss: 44038.4635  Val_Acc: 44.041\n",
      "\n",
      "Epoch 17: Validation loss decreased (44038.463543 --> 41592.193181).  Saving model ...\n",
      "\t Train_Loss: 2497.0925 Train_Acc: 55.640 Val_Loss: 41592.1932  BEST VAL Loss: 41592.1932  Val_Acc: 44.041\n",
      "\n",
      "Epoch 18: Validation loss decreased (41592.193181 --> 39403.291996).  Saving model ...\n",
      "\t Train_Loss: 2365.7439 Train_Acc: 55.158 Val_Loss: 39403.2920  BEST VAL Loss: 39403.2920  Val_Acc: 44.041\n",
      "\n",
      "Epoch 19: Validation loss decreased (39403.291996 --> 37433.225900).  Saving model ...\n",
      "\t Train_Loss: 2247.5289 Train_Acc: 55.708 Val_Loss: 37433.2259  BEST VAL Loss: 37433.2259  Val_Acc: 44.041\n",
      "\n",
      "Epoch 20: Validation loss decreased (37433.225900 --> 35650.752606).  Saving model ...\n",
      "\t Train_Loss: 2140.5733 Train_Acc: 55.777 Val_Loss: 35650.7526  BEST VAL Loss: 35650.7526  Val_Acc: 44.041\n",
      "\n",
      "Epoch 21: Validation loss decreased (35650.752606 --> 34030.377126).  Saving model ...\n",
      "\t Train_Loss: 2043.3807 Train_Acc: 55.915 Val_Loss: 34030.3771  BEST VAL Loss: 34030.3771  Val_Acc: 57.513\n",
      "\n",
      "Epoch 22: Validation loss decreased (34030.377126 --> 32550.830711).  Saving model ...\n",
      "\t Train_Loss: 1954.6029 Train_Acc: 54.402 Val_Loss: 32550.8307  BEST VAL Loss: 32550.8307  Val_Acc: 55.959\n",
      "\n",
      "Epoch 23: Validation loss decreased (32550.830711 --> 31194.577166).  Saving model ...\n",
      "\t Train_Loss: 1873.2204 Train_Acc: 55.708 Val_Loss: 31194.5772  BEST VAL Loss: 31194.5772  Val_Acc: 55.959\n",
      "\n",
      "Epoch 24: Validation loss decreased (31194.577166 --> 29946.822823).  Saving model ...\n",
      "\t Train_Loss: 1798.3494 Train_Acc: 55.021 Val_Loss: 29946.8228  BEST VAL Loss: 29946.8228  Val_Acc: 44.041\n",
      "\n",
      "Epoch 25: Validation loss decreased (29946.822823 --> 28795.048892).  Saving model ...\n",
      "\t Train_Loss: 1729.2363 Train_Acc: 56.052 Val_Loss: 28795.0489  BEST VAL Loss: 28795.0489  Val_Acc: 55.959\n",
      "\n",
      "Epoch 26: Validation loss decreased (28795.048892 --> 27764.228778).  Saving model ...\n",
      "\t Train_Loss: 1687.6046 Train_Acc: 56.052 Val_Loss: 27764.2288  BEST VAL Loss: 27764.2288  Val_Acc: 45.078\n",
      "\n",
      "Epoch 27: Validation loss decreased (27764.228778 --> 26772.731722).  Saving model ...\n",
      "\t Train_Loss: 1661.5538 Train_Acc: 55.433 Val_Loss: 26772.7317  BEST VAL Loss: 26772.7317  Val_Acc: 44.041\n",
      "\n",
      "Epoch 28: Validation loss decreased (26772.731722 --> 25849.588257).  Saving model ...\n",
      "\t Train_Loss: 1617.3909 Train_Acc: 57.015 Val_Loss: 25849.5883  BEST VAL Loss: 25849.5883  Val_Acc: 44.041\n",
      "\n",
      "Epoch 29: Validation loss decreased (25849.588257 --> 24987.959523).  Saving model ...\n",
      "\t Train_Loss: 1564.9345 Train_Acc: 55.502 Val_Loss: 24987.9595  BEST VAL Loss: 24987.9595  Val_Acc: 44.041\n",
      "\n",
      "Epoch 30: Validation loss decreased (24987.959523 --> 24181.925142).  Saving model ...\n",
      "\t Train_Loss: 1514.4971 Train_Acc: 57.359 Val_Loss: 24181.9251  BEST VAL Loss: 24181.9251  Val_Acc: 44.041\n",
      "\n",
      "Epoch 31: Validation loss decreased (24181.925142 --> 23426.261459).  Saving model ...\n",
      "\t Train_Loss: 1467.2179 Train_Acc: 55.433 Val_Loss: 23426.2615  BEST VAL Loss: 23426.2615  Val_Acc: 55.959\n",
      "\n",
      "Epoch 32: Validation loss decreased (23426.261459 --> 22716.396965).  Saving model ...\n",
      "\t Train_Loss: 1422.8011 Train_Acc: 54.470 Val_Loss: 22716.3970  BEST VAL Loss: 22716.3970  Val_Acc: 44.041\n",
      "\n",
      "Epoch 33: Validation loss decreased (22716.396965 --> 22048.290336).  Saving model ...\n",
      "\t Train_Loss: 1380.9954 Train_Acc: 57.221 Val_Loss: 22048.2903  BEST VAL Loss: 22048.2903  Val_Acc: 44.041\n",
      "\n",
      "Epoch 34: Validation loss decreased (22048.290336 --> 21418.359911).  Saving model ...\n",
      "\t Train_Loss: 1341.5807 Train_Acc: 54.677 Val_Loss: 21418.3599  BEST VAL Loss: 21418.3599  Val_Acc: 44.041\n",
      "\n",
      "Epoch 35: Validation loss decreased (21418.359911 --> 20823.426558).  Saving model ...\n",
      "\t Train_Loss: 1304.3516 Train_Acc: 56.809 Val_Loss: 20823.4266  BEST VAL Loss: 20823.4266  Val_Acc: 44.041\n",
      "\n",
      "Epoch 36: Validation loss decreased (20823.426558 --> 20260.654926).  Saving model ...\n",
      "\t Train_Loss: 1269.1384 Train_Acc: 55.365 Val_Loss: 20260.6549  BEST VAL Loss: 20260.6549  Val_Acc: 44.041\n",
      "\n",
      "Epoch 37: Validation loss decreased (20260.654926 --> 19727.516871).  Saving model ...\n",
      "\t Train_Loss: 1235.7790 Train_Acc: 55.640 Val_Loss: 19727.5169  BEST VAL Loss: 19727.5169  Val_Acc: 44.041\n",
      "\n",
      "Epoch 38: Validation loss decreased (19727.516871 --> 19221.706181).  Saving model ...\n",
      "\t Train_Loss: 1204.1319 Train_Acc: 55.365 Val_Loss: 19221.7062  BEST VAL Loss: 19221.7062  Val_Acc: 44.041\n",
      "\n",
      "Epoch 39: Validation loss decreased (19221.706181 --> 18741.182909).  Saving model ...\n",
      "\t Train_Loss: 1174.0658 Train_Acc: 55.571 Val_Loss: 18741.1829  BEST VAL Loss: 18741.1829  Val_Acc: 44.041\n",
      "\n",
      "Epoch 40: Validation loss decreased (18741.182909 --> 18284.100252).  Saving model ...\n",
      "\t Train_Loss: 1145.4657 Train_Acc: 54.883 Val_Loss: 18284.1003  BEST VAL Loss: 18284.1003  Val_Acc: 44.041\n",
      "\n",
      "Epoch 41: Validation loss decreased (18284.100252 --> 17848.783245).  Saving model ...\n",
      "\t Train_Loss: 1118.2275 Train_Acc: 55.158 Val_Loss: 17848.7832  BEST VAL Loss: 17848.7832  Val_Acc: 44.041\n",
      "\n",
      "Epoch 42: Validation loss decreased (17848.783245 --> 17433.713543).  Saving model ...\n",
      "\t Train_Loss: 1092.2560 Train_Acc: 54.883 Val_Loss: 17433.7135  BEST VAL Loss: 17433.7135  Val_Acc: 44.041\n",
      "\n",
      "Epoch 43: Validation loss decreased (17433.713543 --> 17037.512708).  Saving model ...\n",
      "\t Train_Loss: 1067.4653 Train_Acc: 55.296 Val_Loss: 17037.5127  BEST VAL Loss: 17037.5127  Val_Acc: 44.041\n",
      "\n",
      "Epoch 44: Validation loss decreased (17037.512708 --> 16658.916835).  Saving model ...\n",
      "\t Train_Loss: 1043.7771 Train_Acc: 55.296 Val_Loss: 16658.9168  BEST VAL Loss: 16658.9168  Val_Acc: 44.041\n",
      "\n",
      "Epoch 45: Validation loss decreased (16658.916835 --> 16296.782832).  Saving model ...\n",
      "\t Train_Loss: 1021.1164 Train_Acc: 55.708 Val_Loss: 16296.7828  BEST VAL Loss: 16296.7828  Val_Acc: 44.041\n",
      "\n",
      "Epoch 46: Validation loss decreased (16296.782832 --> 15950.057879).  Saving model ...\n",
      "\t Train_Loss: 999.4214 Train_Acc: 54.814 Val_Loss: 15950.0579  BEST VAL Loss: 15950.0579  Val_Acc: 44.041\n",
      "\n",
      "Epoch 47: Validation loss decreased (15950.057879 --> 15617.780925).  Saving model ...\n",
      "\t Train_Loss: 978.6273 Train_Acc: 56.740 Val_Loss: 15617.7809  BEST VAL Loss: 15617.7809  Val_Acc: 44.041\n",
      "\n",
      "Epoch 48: Validation loss decreased (15617.780925 --> 15299.069385).  Saving model ...\n",
      "\t Train_Loss: 958.6850 Train_Acc: 55.296 Val_Loss: 15299.0694  BEST VAL Loss: 15299.0694  Val_Acc: 44.041\n",
      "\n",
      "Epoch 49: Validation loss decreased (15299.069385 --> 14993.112713).  Saving model ...\n",
      "\t Train_Loss: 939.5412 Train_Acc: 55.227 Val_Loss: 14993.1127  BEST VAL Loss: 14993.1127  Val_Acc: 44.041\n",
      "\n",
      "Epoch 50: Validation loss decreased (14993.112713 --> 14699.164248).  Saving model ...\n",
      "\t Train_Loss: 921.1480 Train_Acc: 54.952 Val_Loss: 14699.1642  BEST VAL Loss: 14699.1642  Val_Acc: 44.041\n",
      "\n",
      "Epoch 51: Validation loss decreased (14699.164248 --> 14416.504496).  Saving model ...\n",
      "\t Train_Loss: 903.4624 Train_Acc: 54.814 Val_Loss: 14416.5045  BEST VAL Loss: 14416.5045  Val_Acc: 44.041\n",
      "\n",
      "Epoch 52: Validation loss decreased (14416.504496 --> 14144.508796).  Saving model ...\n",
      "\t Train_Loss: 886.4438 Train_Acc: 55.571 Val_Loss: 14144.5088  BEST VAL Loss: 14144.5088  Val_Acc: 44.041\n",
      "\n",
      "Epoch 53: Validation loss did not decrease\n",
      "\t Train_Loss: 888.7249 Train_Acc: 56.190 Val_Loss: 14371.7670  BEST VAL Loss: 14144.5088  Val_Acc: 44.041\n",
      "\n",
      "Epoch 54: Validation loss decreased (14144.508796 --> 14110.474599).  Saving model ...\n",
      "\t Train_Loss: 972.6091 Train_Acc: 55.365 Val_Loss: 14110.4746  BEST VAL Loss: 14110.4746  Val_Acc: 55.959\n",
      "\n",
      "Epoch 55: Validation loss decreased (14110.474599 --> 13858.517983).  Saving model ...\n",
      "\t Train_Loss: 955.2663 Train_Acc: 55.640 Val_Loss: 13858.5180  BEST VAL Loss: 13858.5180  Val_Acc: 44.041\n",
      "\n",
      "Epoch 56: Validation loss decreased (13858.517983 --> 13615.400959).  Saving model ...\n",
      "\t Train_Loss: 938.5339 Train_Acc: 56.121 Val_Loss: 13615.4010  BEST VAL Loss: 13615.4010  Val_Acc: 44.041\n",
      "\n",
      "Epoch 57: Validation loss decreased (13615.400959 --> 13380.685283).  Saving model ...\n",
      "\t Train_Loss: 922.3763 Train_Acc: 56.671 Val_Loss: 13380.6853  BEST VAL Loss: 13380.6853  Val_Acc: 44.041\n",
      "\n",
      "Epoch 58: Validation loss decreased (13380.685283 --> 13153.907097).  Saving model ...\n",
      "\t Train_Loss: 906.7687 Train_Acc: 54.814 Val_Loss: 13153.9071  BEST VAL Loss: 13153.9071  Val_Acc: 44.041\n",
      "\n",
      "Epoch 59: Validation loss decreased (13153.907097 --> 12934.688227).  Saving model ...\n",
      "\t Train_Loss: 891.6802 Train_Acc: 54.746 Val_Loss: 12934.6882  BEST VAL Loss: 12934.6882  Val_Acc: 44.041\n",
      "\n",
      "Epoch 60: Validation loss decreased (12934.688227 --> 12722.656807).  Saving model ...\n",
      "\t Train_Loss: 877.0863 Train_Acc: 55.089 Val_Loss: 12722.6568  BEST VAL Loss: 12722.6568  Val_Acc: 44.041\n",
      "\n",
      "Epoch 61: Validation loss decreased (12722.656807 --> 12517.464031).  Saving model ...\n",
      "\t Train_Loss: 862.9633 Train_Acc: 55.158 Val_Loss: 12517.4640  BEST VAL Loss: 12517.4640  Val_Acc: 44.041\n",
      "\n",
      "Epoch 62: Validation loss decreased (12517.464031 --> 12318.785196).  Saving model ...\n",
      "\t Train_Loss: 849.2878 Train_Acc: 55.571 Val_Loss: 12318.7852  BEST VAL Loss: 12318.7852  Val_Acc: 44.041\n",
      "\n",
      "Epoch 63: Validation loss decreased (12318.785196 --> 12126.315113).  Saving model ...\n",
      "\t Train_Loss: 836.0396 Train_Acc: 55.708 Val_Loss: 12126.3151  BEST VAL Loss: 12126.3151  Val_Acc: 44.041\n",
      "\n",
      "Epoch 64: Validation loss decreased (12126.315113 --> 11939.784281).  Saving model ...\n",
      "\t Train_Loss: 823.1972 Train_Acc: 56.396 Val_Loss: 11939.7843  BEST VAL Loss: 11939.7843  Val_Acc: 44.041\n",
      "\n",
      "Epoch 65: Validation loss decreased (11939.784281 --> 11758.891233).  Saving model ...\n",
      "\t Train_Loss: 810.7476 Train_Acc: 54.608 Val_Loss: 11758.8912  BEST VAL Loss: 11758.8912  Val_Acc: 44.041\n",
      "\n",
      "Epoch 66: Validation loss decreased (11758.891233 --> 11583.397620).  Saving model ...\n",
      "\t Train_Loss: 798.6688 Train_Acc: 55.227 Val_Loss: 11583.3976  BEST VAL Loss: 11583.3976  Val_Acc: 44.041\n",
      "\n",
      "Epoch 67: Validation loss decreased (11583.397620 --> 11413.063863).  Saving model ...\n",
      "\t Train_Loss: 786.9450 Train_Acc: 55.571 Val_Loss: 11413.0639  BEST VAL Loss: 11413.0639  Val_Acc: 44.041\n",
      "\n",
      "Epoch 68: Validation loss decreased (11413.063863 --> 11247.669394).  Saving model ...\n",
      "\t Train_Loss: 775.5589 Train_Acc: 55.983 Val_Loss: 11247.6694  BEST VAL Loss: 11247.6694  Val_Acc: 44.041\n",
      "\n",
      "Epoch 69: Validation loss decreased (11247.669394 --> 11086.998251).  Saving model ...\n",
      "\t Train_Loss: 764.5008 Train_Acc: 55.571 Val_Loss: 11086.9983  BEST VAL Loss: 11086.9983  Val_Acc: 55.959\n",
      "\n",
      "Epoch 70: Validation loss decreased (11086.998251 --> 10930.855217).  Saving model ...\n",
      "\t Train_Loss: 753.7527 Train_Acc: 56.396 Val_Loss: 10930.8552  BEST VAL Loss: 10930.8552  Val_Acc: 44.041\n",
      "\n",
      "Epoch 71: Validation loss decreased (10930.855217 --> 10779.060561).  Saving model ...\n",
      "\t Train_Loss: 743.3036 Train_Acc: 55.708 Val_Loss: 10779.0606  BEST VAL Loss: 10779.0606  Val_Acc: 44.041\n",
      "\n",
      "Epoch 72: Validation loss decreased (10779.060561 --> 10631.425208).  Saving model ...\n",
      "\t Train_Loss: 733.1418 Train_Acc: 54.333 Val_Loss: 10631.4252  BEST VAL Loss: 10631.4252  Val_Acc: 44.041\n",
      "\n",
      "Epoch 73: Validation loss decreased (10631.425208 --> 10487.777060).  Saving model ...\n",
      "\t Train_Loss: 723.2549 Train_Acc: 54.952 Val_Loss: 10487.7771  BEST VAL Loss: 10487.7771  Val_Acc: 44.041\n",
      "\n",
      "Epoch 74: Validation loss decreased (10487.777060 --> 10347.950716).  Saving model ...\n",
      "\t Train_Loss: 713.6317 Train_Acc: 55.296 Val_Loss: 10347.9507  BEST VAL Loss: 10347.9507  Val_Acc: 44.041\n",
      "\n",
      "Epoch 75: Validation loss decreased (10347.950716 --> 10211.804677).  Saving model ...\n",
      "\t Train_Loss: 704.2611 Train_Acc: 55.227 Val_Loss: 10211.8047  BEST VAL Loss: 10211.8047  Val_Acc: 44.041\n",
      "\n",
      "Epoch 76: Validation loss decreased (10211.804677 --> 10079.194195).  Saving model ...\n",
      "\t Train_Loss: 695.1335 Train_Acc: 55.846 Val_Loss: 10079.1942  BEST VAL Loss: 10079.1942  Val_Acc: 44.041\n",
      "\n",
      "Epoch 77: Validation loss decreased (10079.194195 --> 9949.996268).  Saving model ...\n",
      "\t Train_Loss: 686.2628 Train_Acc: 55.571 Val_Loss: 9949.9963  BEST VAL Loss: 9949.9963  Val_Acc: 44.041\n",
      "\n",
      "Epoch 78: Validation loss decreased (9949.996268 --> 9824.060207).  Saving model ...\n",
      "\t Train_Loss: 677.5955 Train_Acc: 54.814 Val_Loss: 9824.0602  BEST VAL Loss: 9824.0602  Val_Acc: 44.041\n",
      "\n",
      "Epoch 79: Validation loss decreased (9824.060207 --> 9701.280833).  Saving model ...\n",
      "\t Train_Loss: 669.1441 Train_Acc: 55.365 Val_Loss: 9701.2808  BEST VAL Loss: 9701.2808  Val_Acc: 44.041\n",
      "\n",
      "Epoch 80: Validation loss decreased (9701.280833 --> 9581.525154).  Saving model ...\n",
      "\t Train_Loss: 660.9016 Train_Acc: 54.402 Val_Loss: 9581.5252  BEST VAL Loss: 9581.5252  Val_Acc: 44.041\n",
      "\n",
      "Epoch 81: Validation loss decreased (9581.525154 --> 9464.699794).  Saving model ...\n",
      "\t Train_Loss: 652.8600 Train_Acc: 54.058 Val_Loss: 9464.6998  BEST VAL Loss: 9464.6998  Val_Acc: 44.041\n",
      "\n",
      "Epoch 82: Validation loss decreased (9464.699794 --> 9350.689008).  Saving model ...\n",
      "\t Train_Loss: 645.0123 Train_Acc: 54.264 Val_Loss: 9350.6890  BEST VAL Loss: 9350.6890  Val_Acc: 44.041\n",
      "\n",
      "Epoch 83: Validation loss decreased (9350.689008 --> 9239.380853).  Saving model ...\n",
      "\t Train_Loss: 637.3518 Train_Acc: 54.746 Val_Loss: 9239.3809  BEST VAL Loss: 9239.3809  Val_Acc: 44.041\n",
      "\n",
      "Epoch 84: Validation loss decreased (9239.380853 --> 9130.694410).  Saving model ...\n",
      "\t Train_Loss: 629.8699 Train_Acc: 55.777 Val_Loss: 9130.6944  BEST VAL Loss: 9130.6944  Val_Acc: 44.041\n",
      "\n",
      "Epoch 85: Validation loss decreased (9130.694410 --> 9024.532626).  Saving model ...\n",
      "\t Train_Loss: 622.5632 Train_Acc: 54.883 Val_Loss: 9024.5326  BEST VAL Loss: 9024.5326  Val_Acc: 44.041\n",
      "\n",
      "Epoch 86: Validation loss decreased (9024.532626 --> 8920.811455).  Saving model ...\n",
      "\t Train_Loss: 615.4240 Train_Acc: 54.746 Val_Loss: 8920.8115  BEST VAL Loss: 8920.8115  Val_Acc: 44.041\n",
      "\n",
      "Epoch 87: Validation loss decreased (8920.811455 --> 8819.447080).  Saving model ...\n",
      "\t Train_Loss: 608.4469 Train_Acc: 55.089 Val_Loss: 8819.4471  BEST VAL Loss: 8819.4471  Val_Acc: 44.041\n",
      "\n",
      "Epoch 88: Validation loss decreased (8819.447080 --> 8720.360546).  Saving model ...\n",
      "\t Train_Loss: 601.6263 Train_Acc: 55.571 Val_Loss: 8720.3605  BEST VAL Loss: 8720.3605  Val_Acc: 44.041\n",
      "\n",
      "Epoch 89: Validation loss decreased (8720.360546 --> 8623.475581).  Saving model ...\n",
      "\t Train_Loss: 594.9576 Train_Acc: 54.539 Val_Loss: 8623.4756  BEST VAL Loss: 8623.4756  Val_Acc: 44.041\n",
      "\n",
      "Epoch 90: Validation loss decreased (8623.475581 --> 8528.720416).  Saving model ...\n",
      "\t Train_Loss: 588.4340 Train_Acc: 56.602 Val_Loss: 8528.7204  BEST VAL Loss: 8528.7204  Val_Acc: 44.041\n",
      "\n",
      "Epoch 91: Validation loss decreased (8528.720416 --> 8436.025434).  Saving model ...\n",
      "\t Train_Loss: 582.0539 Train_Acc: 54.746 Val_Loss: 8436.0254  BEST VAL Loss: 8436.0254  Val_Acc: 44.041\n",
      "\n",
      "Epoch 92: Validation loss decreased (8436.025434 --> 8345.322891).  Saving model ...\n",
      "\t Train_Loss: 575.8109 Train_Acc: 55.433 Val_Loss: 8345.3229  BEST VAL Loss: 8345.3229  Val_Acc: 55.959\n",
      "\n",
      "Epoch 93: Validation loss decreased (8345.322891 --> 8256.552281).  Saving model ...\n",
      "\t Train_Loss: 569.7003 Train_Acc: 55.433 Val_Loss: 8256.5523  BEST VAL Loss: 8256.5523  Val_Acc: 44.041\n",
      "\n",
      "Epoch 94: Validation loss decreased (8256.552281 --> 8169.649521).  Saving model ...\n",
      "\t Train_Loss: 563.7189 Train_Acc: 55.571 Val_Loss: 8169.6495  BEST VAL Loss: 8169.6495  Val_Acc: 44.041\n",
      "\n",
      "Epoch 95: Validation loss decreased (8169.649521 --> 8084.558161).  Saving model ...\n",
      "\t Train_Loss: 557.8622 Train_Acc: 55.227 Val_Loss: 8084.5582  BEST VAL Loss: 8084.5582  Val_Acc: 44.041\n",
      "\n",
      "Epoch 96: Validation loss decreased (8084.558161 --> 8001.220202).  Saving model ...\n",
      "\t Train_Loss: 552.1262 Train_Acc: 54.470 Val_Loss: 8001.2202  BEST VAL Loss: 8001.2202  Val_Acc: 44.041\n",
      "\n",
      "Epoch 97: Validation loss decreased (8001.220202 --> 7919.593719).  Saving model ...\n",
      "\t Train_Loss: 547.8877 Train_Acc: 57.084 Val_Loss: 7919.5937  BEST VAL Loss: 7919.5937  Val_Acc: 44.041\n",
      "\n",
      "Epoch 98: Validation loss decreased (7919.593719 --> 7839.606510).  Saving model ...\n",
      "\t Train_Loss: 542.3686 Train_Acc: 54.539 Val_Loss: 7839.6065  BEST VAL Loss: 7839.6065  Val_Acc: 44.041\n",
      "\n",
      "Epoch 99: Validation loss decreased (7839.606510 --> 7761.217984).  Saving model ...\n",
      "\t Train_Loss: 536.9597 Train_Acc: 55.640 Val_Loss: 7761.2180  BEST VAL Loss: 7761.2180  Val_Acc: 44.041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "early_stopping_patience = 15\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "valid_loss_min=np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    #TRAINING\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_batch)\n",
    "        y_pred = torch.round(torch.sigmoid(output))\n",
    "        #LOSS\n",
    "        loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() #sum loss for every batch\n",
    "        #ACCURACY\n",
    "        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "        total += y_train_batch.size(0)\n",
    "    train_acc.append(100 * correct / total) #calculate accuracy among all entries in the batches\n",
    "    train_loss.append(running_loss/total_step)  #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "    # VALIDATION\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_valid_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "            batch_loss+=loss_v.item()\n",
    "            #ACCURACY\n",
    "            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "            total_v += y_valid_batch.size(0)\n",
    "        valid_acc.append(100 * correct_v / total_v) \n",
    "        valid_loss.append(batch_loss/total_step_val)\n",
    "    \n",
    "    \n",
    "    if np.mean(valid_loss) <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '/home/bmlserver/jk/iPynb/mmF_Final/savedModel/demo_state_dict.pt')\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n",
    "        valid_loss_min = np.mean(valid_loss)\n",
    "        early_stopping_counter=0 #reset counter if validation loss decreases\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
    "        early_stopping_counter+=1\n",
    "\n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', epoch)\n",
    "        break\n",
    "\n",
    "    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('/home/bmlserver/jk/iPynb/mmF_Final/savedModel/demo_state_dict.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "            X_test_batch = X_test_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_test_batch)\n",
    "            y_pred_prob = torch.sigmoid(output)\n",
    "            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            y_pred_list.append(y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_pred = [a.tolist() for a in y_pred_prob_list]\n",
    "\n",
    "demo_pred_np = np.array(demo_pred)\n",
    "\n",
    "demo_predTT = demo_pred_np.reshape((190, 1))\n",
    "\n",
    "np.savetxt('demo_pred.csv',demo_predTT,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.0]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.0], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.0], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]],\n",
       " [[0.3788445293903351], [0.3788445293903351]]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'demo_predTT' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "demo_predTT\n",
    "\n",
    "%store demo_predTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "# y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c8a573545fa7324502f2b11294db4f50d401dc4d1e743003ac21faacdb8f11f"
  },
  "kernelspec": {
   "display_name": "jk",
   "language": "python",
   "name": "jk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
