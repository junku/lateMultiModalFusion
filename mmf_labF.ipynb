{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "##BONUS: PYTORCH LIGHTNING\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n",
    "\n",
    "seed=42 \n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y, y_pred):\n",
    "    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n",
    "    'Precision':np.round(precision_score(y, y_pred),2),\n",
    "    'Recall':np.round(recall_score(y, y_pred),2),\n",
    "    'F1':np.round(f1_score(y, y_pred),2),\n",
    "    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n",
    "    scores_df = pd.Series(data).to_frame('scores')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_lab.csv')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "train_data = df[df['split_train'] == 1]\n",
    "val_data = df[df['split_val'] == 1]\n",
    "test_data = df[df['split_test']== 1]\n",
    "\n",
    "X = df.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_val = val_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_test = test_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_val = val_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)          \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "comp= [n for n in range(len(cum_sum))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(comp, cum_sum, marker='.')\n",
    "plt.xlabel('PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, X_data, y_data, device=DEVICE):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE)\n",
    "valid_loader = DataLoader(dataset=val_data, batch_size=2)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 42\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_{}\".format(i), 8, 25)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features=out_features\n",
    "    layers.append(nn.Linear(out_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # call the define_model method\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Optimizer and loss definition\n",
    "    lr = trial.suggest_float(\"lr\", 5e-4, 1e-2, log=True)\n",
    "    optimizer =  getattr(optim, 'Adam')(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    # Using the logit binary CE, we include the sigmoid function in the prediction output during the loss calculation\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        #TRAINING\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #sum all batch losses\n",
    "            #ACCURACY\n",
    "            correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(100 * correct / total) \n",
    "        train_loss.append(running_loss/total_step) #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "                #PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                #LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss+=loss_v.item()\n",
    "                #ACCURACY\n",
    "                correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss/total_step_val)\n",
    "\n",
    "        trial.report(np.mean(valid_loss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return np.mean(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-16 16:07:22,093]\u001b[0m A new study created in memory with name: no-name-baac081b-3161-41d1-9c8a-0bbf1d767131\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:07:51,372]\u001b[0m Trial 0 finished with value: 49.79274611398964 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.3705994436131766, 'lr': 0.0008390910942424398}. Best is trial 0 with value: 49.79274611398964.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:19,651]\u001b[0m Trial 1 finished with value: 52.901554404145074 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.2749725331133326, 'lr': 0.004496901944951113}. Best is trial 1 with value: 52.901554404145074.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:52,172]\u001b[0m Trial 2 finished with value: 48.082901554404145 and parameters: {'n_layers': 2, 'n_units_0': 10, 'dropout_0': 0.25014953659985995, 'n_units_1': 24, 'dropout_1': 0.3674878672238483, 'lr': 0.0005154573881141791}. Best is trial 1 with value: 52.901554404145074.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:26,149]\u001b[0m Trial 3 finished with value: 47.64248704663212 and parameters: {'n_layers': 2, 'n_units_0': 21, 'dropout_0': 0.4734119919045249, 'n_units_1': 19, 'dropout_1': 0.31351587705054457, 'lr': 0.0005393100122746635}. Best is trial 1 with value: 52.901554404145074.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:54,607]\u001b[0m Trial 4 finished with value: 51.30829015544041 and parameters: {'n_layers': 1, 'n_units_0': 17, 'dropout_0': 0.24891027361886978, 'lr': 0.000841837292685328}. Best is trial 1 with value: 52.901554404145074.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:58,596]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:31,723]\u001b[0m Trial 6 finished with value: 49.00259067357513 and parameters: {'n_layers': 2, 'n_units_0': 17, 'dropout_0': 0.3398181216000764, 'n_units_1': 11, 'dropout_1': 0.3486568426439701, 'lr': 0.0032132590017274554}. Best is trial 1 with value: 52.901554404145074.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:59,731]\u001b[0m Trial 7 finished with value: 46.28238341968912 and parameters: {'n_layers': 2, 'n_units_0': 18, 'dropout_0': 0.44873726744294923, 'n_units_1': 21, 'dropout_1': 0.2757580424623271, 'lr': 0.00925461358750801}. Best is trial 1 with value: 52.901554404145074.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:26,803]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:30,287]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:57,663]\u001b[0m Trial 10 finished with value: 52.91450777202071 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.20967077410492416, 'lr': 0.002468510784062147}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:25,221]\u001b[0m Trial 11 finished with value: 52.292746113989644 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.2028784088527808, 'lr': 0.002753222441484061}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:53,103]\u001b[0m Trial 12 finished with value: 51.101036269430054 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.3042383930571626, 'lr': 0.004302504140969226}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:57,334]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:25,664]\u001b[0m Trial 14 finished with value: 52.5259067357513 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.2875608613129527, 'lr': 0.0017184325445168405}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:28,457]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:56,425]\u001b[0m Trial 16 finished with value: 50.5958549222798 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.2279411406017654, 'lr': 0.008832784153580799}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:24,088]\u001b[0m Trial 17 finished with value: 51.075129533678755 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.26221745817399966, 'lr': 0.0037286197226935425}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:24,789]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:52,249]\u001b[0m Trial 19 finished with value: 51.26943005181347 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.2289140269849083, 'lr': 0.006771649854418768}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:57,107]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:59,195]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:59,902]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:04,077]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:04,794]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:05,502]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:06,211]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:33,745]\u001b[0m Trial 27 finished with value: 50.155440414507765 and parameters: {'n_layers': 1, 'n_units_0': 8, 'dropout_0': 0.28035120072841463, 'lr': 0.005151980126027458}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:34,446]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:35,155]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:03,101]\u001b[0m Trial 30 finished with value: 52.61658031088084 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.24910235678626447, 'lr': 0.003315216991729541}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:30,978]\u001b[0m Trial 31 finished with value: 52.38341968911917 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.25108089089819724, 'lr': 0.003327945341463427}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:31,692]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:32,414]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:33,130]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:33,844]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:34,553]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:35,353]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:02,115]\u001b[0m Trial 38 finished with value: 51.73575129533678 and parameters: {'n_layers': 1, 'n_units_0': 15, 'dropout_0': 0.2646518293050549, 'lr': 0.002717194812486007}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:02,904]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:03,553]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:28,652]\u001b[0m Trial 41 finished with value: 51.89119170984456 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.24881653059614825, 'lr': 0.0030527620762433394}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:29,300]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:31,198]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:31,842]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:32,482]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:33,199]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:33,833]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:36,975]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:37,620]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:39,511]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:41,403]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:42,044]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:42,681]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:43,297]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:43,906]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:45,791]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:46,433]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:47,076]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:48,953]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:49,576]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:50,205]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:15,004]\u001b[0m Trial 62 finished with value: 52.24093264248705 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.27291303058332456, 'lr': 0.0021891678818591067}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:15,649]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:16,293]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:18,178]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:20,016]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:21,907]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:22,552]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:24,445]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:25,168]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:25,815]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:26,460]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:47,971]\u001b[0m Trial 73 finished with value: 52.215025906735754 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.2615416342303854, 'lr': 0.0028825799505948187}. Best is trial 10 with value: 52.91450777202071.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:48,534]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:50,181]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:51,273]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:51,829]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:53,462]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:55,648]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:56,211]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:56,770]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:58,413]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:00,055]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:00,613]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:21,510]\u001b[0m Trial 85 finished with value: 52.96632124352332 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.21390507154735908, 'lr': 0.0031895485229186484}. Best is trial 85 with value: 52.96632124352332.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:22,075]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:43,773]\u001b[0m Trial 87 finished with value: 52.823834196891184 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.22096366855792277, 'lr': 0.0032219416273794876}. Best is trial 85 with value: 52.96632124352332.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:44,332]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:44,889]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:45,445]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:46,000]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:47,092]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:49,807]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:50,367]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:50,925]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:51,487]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:13,124]\u001b[0m Trial 97 finished with value: 50.10362694300518 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.2114227508163672, 'lr': 0.0006432297849088654}. Best is trial 85 with value: 52.96632124352332.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:13,683]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:14,234]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  76\n",
      "  Number of complete trials:  24\n",
      "Best trial:\n",
      "  Value:  52.96632124352332\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_0: 24\n",
      "    dropout_0: 0.21390507154735908\n",
      "    lr: 0.0031895485229186484\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "\n",
    "params = []\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    params.append(value)\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 24, 0.21390507154735908, 0.0031895485229186484]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = params[0]\n",
    "\n",
    "units_1 = params[1]\n",
    "dropout_1 = np.round(params[2],5)\n",
    "\n",
    "lr = np.round(params[3],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.Linear(X_train.shape[1], units_1)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_1)\n",
    "        self.layer_out = nn.Linear(units_1, 1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.layer_1(inputs))\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer_1): Linear(in_features=42, out_features=24, bias=True)\n",
      "  (dropout1): Dropout(p=0.21391, inplace=False)\n",
      "  (layer_out): Linear(in_features=24, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation loss decreased (inf --> 0.737946).  Saving model ...\n",
      "\t Train_Loss: 0.6270 Train_Acc: 63.824 Val_Loss: 0.7379  BEST VAL Loss: 0.7379  Val_Acc: 45.596\n",
      "\n",
      "Epoch 1: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6100 Train_Acc: 66.713 Val_Loss: 0.7380  BEST VAL Loss: 0.7379  Val_Acc: 47.150\n",
      "\n",
      "Epoch 2: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6009 Train_Acc: 68.432 Val_Loss: 0.7403  BEST VAL Loss: 0.7379  Val_Acc: 48.187\n",
      "\n",
      "Epoch 3: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5945 Train_Acc: 68.638 Val_Loss: 0.7418  BEST VAL Loss: 0.7379  Val_Acc: 47.150\n",
      "\n",
      "Epoch 4: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5896 Train_Acc: 69.188 Val_Loss: 0.7437  BEST VAL Loss: 0.7379  Val_Acc: 47.668\n",
      "\n",
      "Epoch 5: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5854 Train_Acc: 69.051 Val_Loss: 0.7460  BEST VAL Loss: 0.7379  Val_Acc: 48.705\n",
      "\n",
      "Epoch 6: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5817 Train_Acc: 69.670 Val_Loss: 0.7483  BEST VAL Loss: 0.7379  Val_Acc: 49.223\n",
      "\n",
      "Epoch 7: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5783 Train_Acc: 70.426 Val_Loss: 0.7509  BEST VAL Loss: 0.7379  Val_Acc: 48.705\n",
      "\n",
      "Epoch 8: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5751 Train_Acc: 71.045 Val_Loss: 0.7532  BEST VAL Loss: 0.7379  Val_Acc: 50.259\n",
      "\n",
      "Epoch 9: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5724 Train_Acc: 71.527 Val_Loss: 0.7553  BEST VAL Loss: 0.7379  Val_Acc: 50.259\n",
      "\n",
      "Epoch 10: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5696 Train_Acc: 71.527 Val_Loss: 0.7575  BEST VAL Loss: 0.7379  Val_Acc: 50.777\n",
      "\n",
      "Epoch 11: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5671 Train_Acc: 71.733 Val_Loss: 0.7599  BEST VAL Loss: 0.7379  Val_Acc: 50.777\n",
      "\n",
      "Epoch 12: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5646 Train_Acc: 72.215 Val_Loss: 0.7655  BEST VAL Loss: 0.7379  Val_Acc: 50.777\n",
      "\n",
      "Epoch 13: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5621 Train_Acc: 72.902 Val_Loss: 0.7772  BEST VAL Loss: 0.7379  Val_Acc: 49.223\n",
      "\n",
      "Epoch 14: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5598 Train_Acc: 72.765 Val_Loss: 0.7787  BEST VAL Loss: 0.7379  Val_Acc: 50.259\n",
      "\n",
      "Epoch 15: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5575 Train_Acc: 73.040 Val_Loss: 0.7982  BEST VAL Loss: 0.7379  Val_Acc: 50.259\n",
      "\n",
      "Epoch 16: Validation loss did not decrease\n",
      "Early stopped at epoch : 16\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "early_stopping_patience = 15\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "valid_loss_min=np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    #TRAINING\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_batch)\n",
    "        y_pred = torch.round(torch.sigmoid(output))\n",
    "        #LOSS\n",
    "        loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() #sum loss for every batch\n",
    "        #ACCURACY\n",
    "        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "        total += y_train_batch.size(0)\n",
    "    train_acc.append(100 * correct / total) #calculate accuracy among all entries in the batches\n",
    "    train_loss.append(running_loss/total_step)  #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "    # VALIDATION\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_valid_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "            batch_loss+=loss_v.item()\n",
    "            #ACCURACY\n",
    "            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "            total_v += y_valid_batch.size(0)\n",
    "        valid_acc.append(100 * correct_v / total_v) \n",
    "        valid_loss.append(batch_loss/total_step_val)\n",
    "    \n",
    "    \n",
    "    if np.mean(valid_loss) <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '/home/bmlserver/jk/iPynb/mmF_Final/savedModel/lab_state_dict.pt')\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n",
    "        valid_loss_min = np.mean(valid_loss)\n",
    "        early_stopping_counter=0 #reset counter if validation loss decreases\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
    "        early_stopping_counter+=1\n",
    "\n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', epoch)\n",
    "        break\n",
    "\n",
    "    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('/home/bmlserver/jk/iPynb/mmF_Final/savedModel/lab_state_dict.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "            X_test_batch = X_test_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_test_batch)\n",
    "            y_pred_prob = torch.sigmoid(output)\n",
    "            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            y_pred_list.append(y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pred = [a.tolist() for a in y_pred_prob_list]\n",
    "\n",
    "lab_pred_np = np.array(lab_pred)\n",
    "\n",
    "lab_predTT = lab_pred_np.reshape((190, 1))\n",
    "\n",
    "np.savetxt('lab_pred.csv',lab_predTT,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'lab_predTT' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "lab_predTT\n",
    "\n",
    "%store lab_predTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "# y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c8a573545fa7324502f2b11294db4f50d401dc4d1e743003ac21faacdb8f11f"
  },
  "kernelspec": {
   "display_name": "jk",
   "language": "python",
   "name": "jk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
