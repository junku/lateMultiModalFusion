{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "##BONUS: PYTORCH LIGHTNING\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n",
    "\n",
    "seed=42 \n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y, y_pred):\n",
    "    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n",
    "    'Precision':np.round(precision_score(y, y_pred),2),\n",
    "    'Recall':np.round(recall_score(y, y_pred),2),\n",
    "    'F1':np.round(f1_score(y, y_pred),2),\n",
    "    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n",
    "    scores_df = pd.Series(data).to_frame('scores')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_inMed.csv')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "train_data = df[df['split_train'] == 1]\n",
    "val_data = df[df['split_val'] == 1]\n",
    "test_data = df[df['split_test']== 1]\n",
    "\n",
    "X = df.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_val = val_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_test = test_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_val = val_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 618)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)          \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "comp= [n for n in range(len(cum_sum))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(comp, cum_sum, marker='.')\n",
    "plt.xlabel('PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, X_data, y_data, device=DEVICE):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE)\n",
    "valid_loader = DataLoader(dataset=val_data, batch_size=2)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 618\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_{}\".format(i), 8, 25)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features=out_features\n",
    "    layers.append(nn.Linear(out_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # call the define_model method\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Optimizer and loss definition\n",
    "    lr = trial.suggest_float(\"lr\", 5e-4, 1e-2, log=True)\n",
    "    optimizer =  getattr(optim, 'Adam')(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    # Using the logit binary CE, we include the sigmoid function in the prediction output during the loss calculation\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        #TRAINING\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #sum all batch losses\n",
    "            #ACCURACY\n",
    "            correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(100 * correct / total) \n",
    "        train_loss.append(running_loss/total_step) #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "                #PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                #LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss+=loss_v.item()\n",
    "                #ACCURACY\n",
    "                correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss/total_step_val)\n",
    "\n",
    "        trial.report(np.mean(valid_loss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return np.mean(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-16 16:07:18,390]\u001b[0m A new study created in memory with name: no-name-860c4d62-5918-4c9b-aab0-89c09ae68f2b\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:07:45,256]\u001b[0m Trial 0 finished with value: 45.97150259067357 and parameters: {'n_layers': 1, 'n_units_0': 13, 'dropout_0': 0.4816054948216264, 'lr': 0.001346219342174577}. Best is trial 0 with value: 45.97150259067357.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:13,371]\u001b[0m Trial 1 finished with value: 47.759067357512954 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.3849919389248454, 'lr': 0.0019142988219352371}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:42,157]\u001b[0m Trial 2 finished with value: 46.16580310880829 and parameters: {'n_layers': 1, 'n_units_0': 12, 'dropout_0': 0.2891834165797751, 'lr': 0.0010590310580920208}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:13,979]\u001b[0m Trial 3 finished with value: 46.42487046632124 and parameters: {'n_layers': 2, 'n_units_0': 19, 'dropout_0': 0.34882606115924375, 'n_units_1': 23, 'dropout_1': 0.30904726649575204, 'lr': 0.0013552141146866699}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:42,326]\u001b[0m Trial 4 finished with value: 47.61658031088083 and parameters: {'n_layers': 1, 'n_units_0': 17, 'dropout_0': 0.37660260479755925, 'lr': 0.004295708990544834}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:47,383]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:48,109]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:16,483]\u001b[0m Trial 7 finished with value: 46.865284974093264 and parameters: {'n_layers': 1, 'n_units_0': 19, 'dropout_0': 0.3521674938006255, 'lr': 0.002108069906528085}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:17,280]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:18,077]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:18,885]\u001b[0m Trial 10 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:47,002]\u001b[0m Trial 11 finished with value: 46.43782383419689 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.41689085257517583, 'lr': 0.005407349386683133}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:14,963]\u001b[0m Trial 12 finished with value: 47.27979274611399 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.407260351368534, 'lr': 0.004342019786622361}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:42,694]\u001b[0m Trial 13 finished with value: 45.5958549222798 and parameters: {'n_layers': 1, 'n_units_0': 18, 'dropout_0': 0.38811052742874386, 'lr': 0.009061381552138766}. Best is trial 1 with value: 47.759067357512954.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:44,102]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:44,809]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:58,766]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:59,579]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:00,303]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:03,834]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:04,595]\u001b[0m Trial 20 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:05,301]\u001b[0m Trial 21 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:32,955]\u001b[0m Trial 22 finished with value: 48.19948186528497 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.41870240575922335, 'lr': 0.0038386188421703103}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:01,111]\u001b[0m Trial 23 finished with value: 45.712435233160626 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.44732332790969015, 'lr': 0.0033734623338153584}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:01,833]\u001b[0m Trial 24 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:02,544]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:11,646]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:39,525]\u001b[0m Trial 27 finished with value: 46.54145077720207 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.33226567991828126, 'lr': 0.007203626074347812}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:40,242]\u001b[0m Trial 28 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:47,312]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:48,029]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:48,740]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:49,460]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:50,184]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:50,903]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:18,680]\u001b[0m Trial 35 finished with value: 46.48963730569948 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.341872698125502, 'lr': 0.006582638039437701}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:19,383]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:20,082]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:20,877]\u001b[0m Trial 38 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:21,577]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:22,279]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:22,981]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:23,680]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:24,381]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:46,356]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:47,066]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:47,865]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:48,576]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:49,286]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:49,998]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:50,802]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:51,515]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:52,220]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:19,830]\u001b[0m Trial 53 finished with value: 46.30829015544041 and parameters: {'n_layers': 1, 'n_units_0': 19, 'dropout_0': 0.3318151664337085, 'lr': 0.007677640986831395}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:20,532]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:49,476]\u001b[0m Trial 55 finished with value: 45.86787564766839 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.35550194296317295, 'lr': 0.009952765608616609}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:50,245]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:51,014]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:01,211]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:01,954]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:12,145]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:12,883]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:40,816]\u001b[0m Trial 62 finished with value: 46.696891191709845 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.3748723330634781, 'lr': 0.007937339309662884}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:41,539]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:54,860]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:55,503]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:56,146]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:56,788]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:57,435]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:58,087]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:09,420]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:10,059]\u001b[0m Trial 71 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:10,688]\u001b[0m Trial 72 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:35,690]\u001b[0m Trial 73 finished with value: 46.554404145077726 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.336458502513587, 'lr': 0.006622854163009575}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:36,342]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:36,994]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:37,641]\u001b[0m Trial 76 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:38,287]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:38,937]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:39,584]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:40,230]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:40,877]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:41,531]\u001b[0m Trial 82 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:42,234]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:42,882]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:43,543]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:44,199]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:44,857]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:45,669]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:46,361]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:09,622]\u001b[0m Trial 90 finished with value: 45.51813471502591 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.350977848461589, 'lr': 0.009253149854450212}. Best is trial 22 with value: 48.19948186528497.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:10,251]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:10,877]\u001b[0m Trial 92 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:11,485]\u001b[0m Trial 93 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:15,795]\u001b[0m Trial 94 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:16,474]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:17,148]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:17,812]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:18,482]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:19,147]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  82\n",
      "  Number of complete trials:  18\n",
      "Best trial:\n",
      "  Value:  48.19948186528497\n",
      "  Params: \n",
      "    n_layers: 1\n",
      "    n_units_0: 23\n",
      "    dropout_0: 0.41870240575922335\n",
      "    lr: 0.0038386188421703103\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "\n",
    "params = []\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    params.append(value)\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 23, 0.41870240575922335, 0.0038386188421703103]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = params[0]\n",
    "\n",
    "units_1 = params[1]\n",
    "dropout_1 = np.round(params[2],5)\n",
    "\n",
    "lr = np.round(params[3],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.Linear(X_train.shape[1], units_1)\n",
    "        self.layer_out = nn.Linear(units_1, 1) \n",
    "        self.dropout1 = nn.Dropout(p=dropout_1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.layer_1(inputs))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer_1): Linear(in_features=618, out_features=23, bias=True)\n",
      "  (layer_out): Linear(in_features=23, out_features=1, bias=True)\n",
      "  (dropout1): Dropout(p=0.4187, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation loss decreased (inf --> 0.870722).  Saving model ...\n",
      "\t Train_Loss: 0.7037 Train_Acc: 63.549 Val_Loss: 0.8707  BEST VAL Loss: 0.8707  Val_Acc: 44.560\n",
      "\n",
      "Epoch 1: Validation loss decreased (0.870722 --> 0.826017).  Saving model ...\n",
      "\t Train_Loss: 0.6694 Train_Acc: 65.750 Val_Loss: 0.8260  BEST VAL Loss: 0.8260  Val_Acc: 46.114\n",
      "\n",
      "Epoch 2: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6476 Train_Acc: 67.744 Val_Loss: 0.8306  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 3: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6278 Train_Acc: 70.220 Val_Loss: 0.8361  BEST VAL Loss: 0.8260  Val_Acc: 48.187\n",
      "\n",
      "Epoch 4: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6132 Train_Acc: 70.495 Val_Loss: 0.8463  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 5: Validation loss did not decrease\n",
      "\t Train_Loss: 0.6026 Train_Acc: 70.633 Val_Loss: 0.8562  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 6: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5946 Train_Acc: 71.114 Val_Loss: 0.8688  BEST VAL Loss: 0.8260  Val_Acc: 47.150\n",
      "\n",
      "Epoch 7: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5896 Train_Acc: 71.458 Val_Loss: 0.8827  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 8: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5835 Train_Acc: 71.664 Val_Loss: 0.8830  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 9: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5771 Train_Acc: 72.008 Val_Loss: 0.8842  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 10: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5739 Train_Acc: 71.114 Val_Loss: 0.8854  BEST VAL Loss: 0.8260  Val_Acc: 47.150\n",
      "\n",
      "Epoch 11: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5683 Train_Acc: 72.490 Val_Loss: 0.8915  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 12: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5633 Train_Acc: 72.696 Val_Loss: 0.8990  BEST VAL Loss: 0.8260  Val_Acc: 48.187\n",
      "\n",
      "Epoch 13: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5601 Train_Acc: 71.871 Val_Loss: 0.9067  BEST VAL Loss: 0.8260  Val_Acc: 47.150\n",
      "\n",
      "Epoch 14: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5579 Train_Acc: 72.283 Val_Loss: 0.9183  BEST VAL Loss: 0.8260  Val_Acc: 47.668\n",
      "\n",
      "Epoch 15: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5558 Train_Acc: 72.146 Val_Loss: 0.9322  BEST VAL Loss: 0.8260  Val_Acc: 48.705\n",
      "\n",
      "Epoch 16: Validation loss did not decrease\n",
      "\t Train_Loss: 0.5531 Train_Acc: 72.765 Val_Loss: 0.9409  BEST VAL Loss: 0.8260  Val_Acc: 48.187\n",
      "\n",
      "Epoch 17: Validation loss did not decrease\n",
      "Early stopped at epoch : 17\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "early_stopping_patience = 15\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "valid_loss_min=np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    #TRAINING\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_batch)\n",
    "        y_pred = torch.round(torch.sigmoid(output))\n",
    "        #LOSS\n",
    "        loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() #sum loss for every batch\n",
    "        #ACCURACY\n",
    "        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "        total += y_train_batch.size(0)\n",
    "    train_acc.append(100 * correct / total) #calculate accuracy among all entries in the batches\n",
    "    train_loss.append(running_loss/total_step)  #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "    # VALIDATION\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_valid_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "            batch_loss+=loss_v.item()\n",
    "            #ACCURACY\n",
    "            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "            total_v += y_valid_batch.size(0)\n",
    "        valid_acc.append(100 * correct_v / total_v) \n",
    "        valid_loss.append(batch_loss/total_step_val)\n",
    "    \n",
    "    \n",
    "    if np.mean(valid_loss) <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '/home/bmlserver/jk/iPynb/mmF_Final/savedModel/inMed_state_dict.pt')\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n",
    "        valid_loss_min = np.mean(valid_loss)\n",
    "        early_stopping_counter=0 #reset counter if validation loss decreases\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
    "        early_stopping_counter+=1\n",
    "\n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', epoch)\n",
    "        break\n",
    "\n",
    "    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('/home/bmlserver/jk/iPynb/mmF_Final/savedModel/inMed_state_dict.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "            X_test_batch = X_test_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_test_batch)\n",
    "            y_pred_prob = torch.sigmoid(output)\n",
    "            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            y_pred_list.append(y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inMed_pred = [a.tolist() for a in y_pred_prob_list]\n",
    "\n",
    "inMed_pred_np = np.array(inMed_pred)\n",
    "\n",
    "inMed_predTT = inMed_pred_np.reshape((190, 1))\n",
    "\n",
    "np.savetxt('inMed_pred.csv',inMed_predTT,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'inMed_predTT' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "inMed_predTT\n",
    "\n",
    "%store inMed_predTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "# y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c8a573545fa7324502f2b11294db4f50d401dc4d1e743003ac21faacdb8f11f"
  },
  "kernelspec": {
   "display_name": "jk",
   "language": "python",
   "name": "jk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
