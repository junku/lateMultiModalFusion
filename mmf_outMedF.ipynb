{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "##BONUS: PYTORCH LIGHTNING\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, auc, roc_curve, accuracy_score\n",
    "\n",
    "seed=42 \n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y, y_pred):\n",
    "    data={'Accuracy': np.round(accuracy_score(y, y_pred),2),\n",
    "    'Precision':np.round(precision_score(y, y_pred),2),\n",
    "    'Recall':np.round(recall_score(y, y_pred),2),\n",
    "    'F1':np.round(f1_score(y, y_pred),2),\n",
    "    'ROC AUC':np.round(roc_auc_score(y, y_pred),2)}\n",
    "    scores_df = pd.Series(data).to_frame('scores')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_outMed.csv')\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "train_data = df[df['split_train'] == 1]\n",
    "val_data = df[df['split_val'] == 1]\n",
    "test_data = df[df['split_test']== 1]\n",
    "\n",
    "X = df.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_val = val_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "X_test = test_data.drop(columns=['label', 'split_test', 'split_train', 'split_val', 'pe_type_central', 'pe_type_segmental', 'pe_type_subsegmental', 'pred', 'Unnamed: 0', 'idx'], axis=1)\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_val = val_data['label']\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 566)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()   \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)          \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "comp= [n for n in range(len(cum_sum))]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(comp, cum_sum, marker='.')\n",
    "plt.xlabel('PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, X_data, y_data, device=DEVICE):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data \n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCHSIZE)\n",
    "valid_loader = DataLoader(dataset=val_data, batch_size=2)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 566\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_{}\".format(i), 8, 25)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "        in_features=out_features\n",
    "    layers.append(nn.Linear(out_features, 1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # call the define_model method\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Optimizer and loss definition\n",
    "    lr = trial.suggest_float(\"lr\", 5e-4, 1e-2, log=True)\n",
    "    optimizer =  getattr(optim, 'Adam')(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss() \n",
    "    # Using the logit binary CE, we include the sigmoid function in the prediction output during the loss calculation\n",
    "    \n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    \n",
    "    valid_acc = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    total_step = len(train_loader)\n",
    "    total_step_val = len(valid_loader)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        \n",
    "        #TRAINING\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_train_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss+=loss.item() #sum all batch losses\n",
    "            #ACCURACY\n",
    "            correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "            total += y_train_batch.size(0)\n",
    "        train_acc.append(100 * correct / total) \n",
    "        train_loss.append(running_loss/total_step) #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "        # VALIDATION\n",
    "        correct_v = 0\n",
    "        total_v = 0\n",
    "        batch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "                X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "                #PREDICTION\n",
    "                output = model(X_valid_batch)\n",
    "                y_pred = torch.round(torch.sigmoid(output))\n",
    "                #LOSS\n",
    "                loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "                batch_loss+=loss_v.item()\n",
    "                #ACCURACY\n",
    "                correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "                total_v += y_valid_batch.size(0)\n",
    "            valid_acc.append(100 * correct_v / total_v)\n",
    "            valid_loss.append(batch_loss/total_step_val)\n",
    "\n",
    "        trial.report(np.mean(valid_loss), epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return np.mean(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-16 16:07:25,360]\u001b[0m A new study created in memory with name: no-name-16e2b5a6-6ffd-4ab6-bad2-67e55518f0ae\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:07:54,474]\u001b[0m Trial 0 finished with value: 44.01554404145078 and parameters: {'n_layers': 1, 'n_units_0': 11, 'dropout_0': 0.40068195197230927, 'lr': 0.00101079808665992}. Best is trial 0 with value: 44.01554404145078.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:22,573]\u001b[0m Trial 1 finished with value: 44.22279792746114 and parameters: {'n_layers': 1, 'n_units_0': 17, 'dropout_0': 0.4187060264786149, 'lr': 0.002717749093192317}. Best is trial 1 with value: 44.22279792746114.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:08:54,369]\u001b[0m Trial 2 finished with value: 45.16839378238342 and parameters: {'n_layers': 2, 'n_units_0': 8, 'dropout_0': 0.2615661636286679, 'n_units_1': 8, 'dropout_1': 0.25769742940173246, 'lr': 0.009221394178951528}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:25,989]\u001b[0m Trial 3 finished with value: 44.10621761658032 and parameters: {'n_layers': 2, 'n_units_0': 16, 'dropout_0': 0.4501712977568432, 'n_units_1': 10, 'dropout_1': 0.4610328631293638, 'lr': 0.0010442315161842894}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:57,421]\u001b[0m Trial 4 finished with value: 44.520725388601036 and parameters: {'n_layers': 2, 'n_units_0': 14, 'dropout_0': 0.2610874963755459, 'n_units_1': 12, 'dropout_1': 0.3490127819079101, 'lr': 0.000614022325595852}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:09:58,218]\u001b[0m Trial 5 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:00,335]\u001b[0m Trial 6 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:01,129]\u001b[0m Trial 7 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:01,924]\u001b[0m Trial 8 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:02,722]\u001b[0m Trial 9 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:30,898]\u001b[0m Trial 10 finished with value: 44.30051813471503 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.33431673681003204, 'lr': 0.009170446237111652}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:31,711]\u001b[0m Trial 11 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:32,527]\u001b[0m Trial 12 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:33,349]\u001b[0m Trial 13 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:34,170]\u001b[0m Trial 14 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:34,980]\u001b[0m Trial 15 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:35,791]\u001b[0m Trial 16 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:36,506]\u001b[0m Trial 17 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:37,319]\u001b[0m Trial 18 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:10:38,133]\u001b[0m Trial 19 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:06,292]\u001b[0m Trial 20 finished with value: 44.80569948186528 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.33514569985731657, 'lr': 0.00355347251949929}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:34,071]\u001b[0m Trial 21 finished with value: 43.29015544041451 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.32580102393510985, 'lr': 0.006922843151645584}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:34,778]\u001b[0m Trial 22 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:11:35,479]\u001b[0m Trial 23 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:03,143]\u001b[0m Trial 24 finished with value: 44.83160621761658 and parameters: {'n_layers': 1, 'n_units_0': 19, 'dropout_0': 0.23186424728423496, 'lr': 0.003662302204356889}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:26,341]\u001b[0m Trial 25 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:12:47,155]\u001b[0m Trial 26 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:13,231]\u001b[0m Trial 27 finished with value: 43.5880829015544 and parameters: {'n_layers': 1, 'n_units_0': 19, 'dropout_0': 0.3863173767456285, 'lr': 0.008066476529621468}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:39,614]\u001b[0m Trial 28 finished with value: 43.976683937823836 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.3099429437459127, 'lr': 0.005224153268321135}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:41,726]\u001b[0m Trial 29 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:43,142]\u001b[0m Trial 30 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:43,943]\u001b[0m Trial 31 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:46,057]\u001b[0m Trial 32 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:46,780]\u001b[0m Trial 33 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:47,594]\u001b[0m Trial 34 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:49,006]\u001b[0m Trial 35 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:49,796]\u001b[0m Trial 36 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:13:50,585]\u001b[0m Trial 37 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:18,422]\u001b[0m Trial 38 finished with value: 43.6139896373057 and parameters: {'n_layers': 1, 'n_units_0': 22, 'dropout_0': 0.3638403901240344, 'lr': 0.00751393389579246}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:19,211]\u001b[0m Trial 39 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:20,002]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:14:47,401]\u001b[0m Trial 41 finished with value: 44.44300518134715 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.33551738365508, 'lr': 0.009776635702631339}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:15,044]\u001b[0m Trial 42 finished with value: 43.976683937823836 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.3152455105858129, 'lr': 0.008179060564222886}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:42,691]\u001b[0m Trial 43 finished with value: 44.4559585492228 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.344372923102957, 'lr': 0.009950006769459582}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:15:46,207]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:14,160]\u001b[0m Trial 45 finished with value: 44.766839378238345 and parameters: {'n_layers': 1, 'n_units_0': 18, 'dropout_0': 0.25734959831781334, 'lr': 0.00857980970612899}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:14,877]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:15,681]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:16,392]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:17,198]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:18,009]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:16:45,673]\u001b[0m Trial 51 finished with value: 44.85751295336787 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.2980633594147913, 'lr': 0.008867044169854265}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:11,514]\u001b[0m Trial 52 finished with value: 43.80829015544042 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.2921612027448521, 'lr': 0.00855812169028981}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:12,153]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:37,207]\u001b[0m Trial 54 finished with value: 44.766839378238345 and parameters: {'n_layers': 1, 'n_units_0': 20, 'dropout_0': 0.30354276094717686, 'lr': 0.007656450090898064}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:17:56,441]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:21,126]\u001b[0m Trial 56 finished with value: 43.432642487046635 and parameters: {'n_layers': 1, 'n_units_0': 21, 'dropout_0': 0.30481349171984035, 'lr': 0.007624810344768649}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:23,019]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:23,666]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:27,451]\u001b[0m Trial 59 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:32,909]\u001b[0m Trial 60 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:33,467]\u001b[0m Trial 61 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:34,031]\u001b[0m Trial 62 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:34,592]\u001b[0m Trial 63 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:35,153]\u001b[0m Trial 64 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:35,779]\u001b[0m Trial 65 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:36,334]\u001b[0m Trial 66 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:36,892]\u001b[0m Trial 67 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:37,521]\u001b[0m Trial 68 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:38,625]\u001b[0m Trial 69 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:18:39,256]\u001b[0m Trial 70 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:00,637]\u001b[0m Trial 71 finished with value: 43.601036269430054 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.34339624594974416, 'lr': 0.009952026441227262}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:22,288]\u001b[0m Trial 72 finished with value: 44.067357512953365 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.3346405449797696, 'lr': 0.009695773129381345}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:22,853]\u001b[0m Trial 73 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:23,415]\u001b[0m Trial 74 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:23,964]\u001b[0m Trial 75 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:45,618]\u001b[0m Trial 76 finished with value: 44.067357512953365 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.38215953263113445, 'lr': 0.007497934893188879}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:46,173]\u001b[0m Trial 77 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:46,730]\u001b[0m Trial 78 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:47,356]\u001b[0m Trial 79 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:47,915]\u001b[0m Trial 80 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:19:49,012]\u001b[0m Trial 81 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:10,644]\u001b[0m Trial 82 finished with value: 44.31347150259067 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.2587113748470902, 'lr': 0.009882183349045632}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:11,214]\u001b[0m Trial 83 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:11,774]\u001b[0m Trial 84 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:13,952]\u001b[0m Trial 85 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:14,472]\u001b[0m Trial 86 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:15,067]\u001b[0m Trial 87 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:15,561]\u001b[0m Trial 88 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:16,056]\u001b[0m Trial 89 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:16,552]\u001b[0m Trial 90 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:17,048]\u001b[0m Trial 91 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:35,589]\u001b[0m Trial 92 finished with value: 44.20984455958549 and parameters: {'n_layers': 1, 'n_units_0': 24, 'dropout_0': 0.26069293483101197, 'lr': 0.009726346630057737}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:20:54,495]\u001b[0m Trial 93 finished with value: 43.976683937823836 and parameters: {'n_layers': 1, 'n_units_0': 25, 'dropout_0': 0.243883846545549, 'lr': 0.008452486178770492}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:13,151]\u001b[0m Trial 94 finished with value: 43.44559585492228 and parameters: {'n_layers': 1, 'n_units_0': 23, 'dropout_0': 0.23255397974729877, 'lr': 0.009966459386468458}. Best is trial 2 with value: 45.16839378238342.\u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:14,113]\u001b[0m Trial 95 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:14,599]\u001b[0m Trial 96 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:15,178]\u001b[0m Trial 97 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:15,668]\u001b[0m Trial 98 pruned. \u001b[0m\n",
      "\u001b[32m[I 2022-04-16 16:21:16,627]\u001b[0m Trial 99 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  100\n",
      "  Number of pruned trials:  73\n",
      "  Number of complete trials:  27\n",
      "Best trial:\n",
      "  Value:  45.16839378238342\n",
      "  Params: \n",
      "    n_layers: 2\n",
      "    n_units_0: 8\n",
      "    dropout_0: 0.2615661636286679\n",
      "    n_units_1: 8\n",
      "    dropout_1: 0.25769742940173246\n",
      "    lr: 0.009221394178951528\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "\n",
    "params = []\n",
    "\n",
    "for key, value in trial.params.items():\n",
    "    params.append(value)\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 8, 0.2615661636286679, 8, 0.25769742940173246, 0.009221394178951528]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = params[0]\n",
    "\n",
    "units_1 = params[1]\n",
    "dropout_1 = np.round(params[2],5)\n",
    "\n",
    "lr = np.round(params[3],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1 = nn.Linear(X_train.shape[1], units_1)\n",
    "        self.layer_out = nn.Linear(units_1, 1) \n",
    "        self.dropout1 = nn.Dropout(p=dropout_1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.layer_1(inputs))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer_1): Linear(in_features=566, out_features=8, bias=True)\n",
      "  (layer_out): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (dropout1): Dropout(p=0.26157, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Validation loss decreased (inf --> 156721.672863).  Saving model ...\n",
      "\t Train_Loss: 125596.6049 Train_Acc: 54.333 Val_Loss: 156721.6729  BEST VAL Loss: 156721.6729  Val_Acc: 39.378\n",
      "\n",
      "Epoch 1: Validation loss decreased (156721.672863 --> 103963.336346).  Saving model ...\n",
      "\t Train_Loss: 101606.0892 Train_Acc: 56.809 Val_Loss: 103963.3363  BEST VAL Loss: 103963.3363  Val_Acc: 37.824\n",
      "\n",
      "Epoch 2: Validation loss decreased (103963.336346 --> 94196.020175).  Saving model ...\n",
      "\t Train_Loss: 81355.0529 Train_Acc: 56.465 Val_Loss: 94196.0202  BEST VAL Loss: 94196.0202  Val_Acc: 43.523\n",
      "\n",
      "Epoch 3: Validation loss decreased (94196.020175 --> 93778.584615).  Saving model ...\n",
      "\t Train_Loss: 74659.6546 Train_Acc: 56.121 Val_Loss: 93778.5846  BEST VAL Loss: 93778.5846  Val_Acc: 42.487\n",
      "\n",
      "Epoch 4: Validation loss decreased (93778.584615 --> 86560.578870).  Saving model ...\n",
      "\t Train_Loss: 64710.0351 Train_Acc: 57.634 Val_Loss: 86560.5789  BEST VAL Loss: 86560.5789  Val_Acc: 40.933\n",
      "\n",
      "Epoch 5: Validation loss decreased (86560.578870 --> 84914.218614).  Saving model ...\n",
      "\t Train_Loss: 55162.6281 Train_Acc: 58.047 Val_Loss: 84914.2186  BEST VAL Loss: 84914.2186  Val_Acc: 40.415\n",
      "\n",
      "Epoch 6: Validation loss did not decrease\n",
      "\t Train_Loss: 49125.6334 Train_Acc: 57.634 Val_Loss: 86406.3612  BEST VAL Loss: 84914.2186  Val_Acc: 42.487\n",
      "\n",
      "Epoch 7: Validation loss did not decrease\n",
      "\t Train_Loss: 43720.4803 Train_Acc: 58.184 Val_Loss: 91230.6423  BEST VAL Loss: 84914.2186  Val_Acc: 43.523\n",
      "\n",
      "Epoch 8: Validation loss did not decrease\n",
      "\t Train_Loss: 41425.7181 Train_Acc: 57.909 Val_Loss: 112530.8943  BEST VAL Loss: 84914.2186  Val_Acc: 41.969\n",
      "\n",
      "Epoch 9: Validation loss did not decrease\n",
      "\t Train_Loss: 37622.7379 Train_Acc: 58.459 Val_Loss: 118350.8708  BEST VAL Loss: 84914.2186  Val_Acc: 41.969\n",
      "\n",
      "Epoch 10: Validation loss did not decrease\n",
      "\t Train_Loss: 34748.3229 Train_Acc: 57.634 Val_Loss: 131143.7179  BEST VAL Loss: 84914.2186  Val_Acc: 43.523\n",
      "\n",
      "Epoch 11: Validation loss did not decrease\n",
      "\t Train_Loss: 31914.4629 Train_Acc: 59.354 Val_Loss: 135508.7686  BEST VAL Loss: 84914.2186  Val_Acc: 41.451\n",
      "\n",
      "Epoch 12: Validation loss did not decrease\n",
      "\t Train_Loss: 29583.4924 Train_Acc: 57.978 Val_Loss: 138112.2315  BEST VAL Loss: 84914.2186  Val_Acc: 40.933\n",
      "\n",
      "Epoch 13: Validation loss did not decrease\n",
      "\t Train_Loss: 27588.2875 Train_Acc: 59.147 Val_Loss: 139397.0852  BEST VAL Loss: 84914.2186  Val_Acc: 43.523\n",
      "\n",
      "Epoch 14: Validation loss did not decrease\n",
      "\t Train_Loss: 25764.3716 Train_Acc: 58.047 Val_Loss: 137226.1105  BEST VAL Loss: 84914.2186  Val_Acc: 42.487\n",
      "\n",
      "Epoch 15: Validation loss did not decrease\n",
      "\t Train_Loss: 24158.0994 Train_Acc: 58.735 Val_Loss: 133575.3359  BEST VAL Loss: 84914.2186  Val_Acc: 42.487\n",
      "\n",
      "Epoch 16: Validation loss did not decrease\n",
      "\t Train_Loss: 22751.1358 Train_Acc: 57.703 Val_Loss: 130519.3488  BEST VAL Loss: 84914.2186  Val_Acc: 44.041\n",
      "\n",
      "Epoch 17: Validation loss did not decrease\n",
      "\t Train_Loss: 21955.8447 Train_Acc: 56.327 Val_Loss: 128574.9873  BEST VAL Loss: 84914.2186  Val_Acc: 44.560\n",
      "\n",
      "Epoch 18: Validation loss did not decrease\n",
      "\t Train_Loss: 20916.3328 Train_Acc: 57.978 Val_Loss: 127215.0446  BEST VAL Loss: 84914.2186  Val_Acc: 43.005\n",
      "\n",
      "Epoch 19: Validation loss did not decrease\n",
      "\t Train_Loss: 20222.8649 Train_Acc: 55.846 Val_Loss: 127570.7654  BEST VAL Loss: 84914.2186  Val_Acc: 44.041\n",
      "\n",
      "Epoch 20: Validation loss did not decrease\n",
      "\t Train_Loss: 19287.3746 Train_Acc: 57.015 Val_Loss: 125077.5206  BEST VAL Loss: 84914.2186  Val_Acc: 42.487\n",
      "\n",
      "Epoch 21: Validation loss did not decrease\n",
      "Early stopped at epoch : 21\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "early_stopping_patience = 15\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "valid_loss_min=np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss=0\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    #TRAINING\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_batch)\n",
    "        y_pred = torch.round(torch.sigmoid(output))\n",
    "        #LOSS\n",
    "        loss = criterion(output, y_train_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=loss.item() #sum loss for every batch\n",
    "        #ACCURACY\n",
    "        correct += torch.sum(y_pred==y_train_batch.unsqueeze(1)).item()\n",
    "        total += y_train_batch.size(0)\n",
    "    train_acc.append(100 * correct / total) #calculate accuracy among all entries in the batches\n",
    "    train_loss.append(running_loss/total_step)  #get average loss among all batches dividing total loss by the number of batches\n",
    "\n",
    "    # VALIDATION\n",
    "    correct_v = 0\n",
    "    total_v = 0\n",
    "    batch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_valid_batch, y_valid_batch) in enumerate(valid_loader):\n",
    "            X_valid_batch,y_valid_batch=X_valid_batch.to(DEVICE),y_valid_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_valid_batch)\n",
    "            y_pred = torch.round(torch.sigmoid(output))\n",
    "            #LOSS\n",
    "            loss_v = criterion(output, y_valid_batch.unsqueeze(1))\n",
    "            batch_loss+=loss_v.item()\n",
    "            #ACCURACY\n",
    "            correct_v += torch.sum(y_pred==y_valid_batch.unsqueeze(1)).item()\n",
    "            total_v += y_valid_batch.size(0)\n",
    "        valid_acc.append(100 * correct_v / total_v) \n",
    "        valid_loss.append(batch_loss/total_step_val)\n",
    "    \n",
    "    \n",
    "    if np.mean(valid_loss) <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '/home/bmlserver/jk/iPynb/mmF_Final/savedModel/outMed_state_dict.pt')\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss decreased ({valid_loss_min:.6f} --> {np.mean(valid_loss):.6f}).  Saving model ...')\n",
    "        valid_loss_min = np.mean(valid_loss)\n",
    "        early_stopping_counter=0 #reset counter if validation loss decreases\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 0:01}: Validation loss did not decrease')\n",
    "        early_stopping_counter+=1\n",
    "\n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', epoch)\n",
    "        break\n",
    "\n",
    "    print(f'\\t Train_Loss: {np.mean(train_loss):.4f} Train_Acc: {(100 * correct / total):.3f} Val_Loss: {np.mean(valid_loss):.4f}  BEST VAL Loss: {valid_loss_min:.4f}  Val_Acc: {(100 * correct_v / total_v):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "\n",
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('/home/bmlserver/jk/iPynb/mmF_Final/savedModel/outMed_state_dict.pt'))\n",
    "\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_idx, (X_test_batch, y_test_batch) in enumerate(test_loader):\n",
    "            X_test_batch = X_test_batch.to(DEVICE)\n",
    "            #PREDICTION\n",
    "            output = model(X_test_batch)\n",
    "            y_pred_prob = torch.sigmoid(output)\n",
    "            y_pred_prob_list.append(y_pred_prob.cpu().numpy())\n",
    "            y_pred = torch.round(y_pred_prob)\n",
    "            y_pred_list.append(y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outMed_pred = [a.tolist() for a in y_pred_prob_list]\n",
    "\n",
    "outMed_pred_np = np.array(outMed_pred)\n",
    "\n",
    "outMed_predTT = outMed_pred_np.reshape((190, 1))\n",
    "\n",
    "np.savetxt('outMed_pred.csv',outMed_predTT,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'outMed_predTT' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "# outMed_predTT\n",
    "\n",
    "# %store outMed_predTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_prob_list = [a.squeeze().tolist() for a in y_pred_prob_list]\n",
    "# y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c8a573545fa7324502f2b11294db4f50d401dc4d1e743003ac21faacdb8f11f"
  },
  "kernelspec": {
   "display_name": "jk",
   "language": "python",
   "name": "jk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
